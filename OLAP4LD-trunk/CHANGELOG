This is the changelog of OLAP4LD.

==2015-05-27==
* Made it work with Java 8 (only had to add some empty inherited methods)
* Started refining the test cases (had to remove rounding since there were math problems at: CellSet.

==2014-10-19==
* Refined test cases for EKAW paper for estimating size of Global Cube, see http://linked-data-cubes.org/index.php/Global_Cube_Evaluation_EKAW14#Program_for_computing_the_number_of_derived_data_cubes_in_case_conversion_correspondences_are_seen_as_merging_correspondences_and_no_cycles_.28repeated_application_of_correspondences.29_are_allowed

In ISEM paper, we compare: Percentage of Nos with Real GDP growth rate.

Since we cannot compute Percentage of Nos, we compare:

SUM of Nos from GESIS with Employment rate, by sex from Estatwrap from Germany.

This would show: Integration of Germany. 

In ISEM paper, we had with respect to size of datasets:

1) 2 DS with 20,268 triples
3a) 4 DS with 24,636 triples
3b) 8 DS with 35,482 triples

We now would like to extend this.


==2014-09-22==
* Refined test cases for EKAW paper, see http://linked-data-cubes.org/index.php/Global_Cube_Evaluation_EKAW14
* tagged version
* data added to ekaw version

==2014-09-21==
* Fixed loading problem:

The problem was still the loading of resources and setting of setLoaded. 

How do want to have it?

We have a URI of a cube that we want to load.

We check whether already loaded: 1) Is URI loaded? OR 2) Is Location loaded?

If yes, we set: setLoaded(URI), setLoaded(Location).

If not, we load: loadCube()

For every resource: 1) Is URI loaded? OR 2) Is Location loaded?

If yes, we set: setLoaded(URI), setLoaded(Location).

If not, we load: loadInStore(URI)

After loading, we set it loaded: setLoaded(URI), setLoaded(Location).

After we loaded the cube, we set it loaded: setLoaded(URI), setLoaded(Location).

Note: We assume that if a dataset is loaded, then the full directed crawling was executed. We make that sure by not simply crawling other datasets in the direct crawl.

Also, had to change from URL to Sring (!)

loadedMap.put(resource.toString().hashCode(), true);

==2014-09-20==
* Now, BaseCube-Derived returns all measures, but not if asked for data.
* Now, if we run BaseCube directly after Drill-Across, we would also
run OLAP2SPARQL.
* Also, we make sure we also call projection.

Now, we:

* automatically run createOlap2SparqlIterator if we directly ask for a BaseCube in case of Drill-Across and directly.

Better in our case of Convert-Cube:

* We do a projection operator. 

* It works, but we have the problem that: If we ask a query for GDP per Capita, we will not... wait, yes, if we first do a projection. 

* We may have introduced some error with counting, but the test
cases for Convert-Cube and Example generally work: 

Status:

Example_QB_Datasets_QueryTest.java - *basic MDX examples* - yes
Example_QB_Datasets_DrillAcross_QueryTest.java *basic MDX drill-across examples* - yes 
Example_QB_Datasets_ConvertCube_QueryTest.java *MDX with correspondences examples* - not implemented, yet
LDCX_Performance_Evaluation_ConvertCubeTest.java *Manual Logical Query Plan with Convert-Cube examples* - yes


==2014-09-19==
* I need to make all test cases work, again.

* Failed 
testExampleEdgarCOSTCOOlapTypicalCrossjoin1
testExampleEdgarCOSTCOOlapLargeDateCrossjoinColumnsNonEmptyRowsCOUNT
testExampleEdgarCOSTCOOlapLargeSegmentSubjectCrossjoinNonEmptyRowsCOUNT
testOriginalEdgarCOSTCOOlapBothMeasuresPlusSubject
testExampleEstatwrapGDPpercapitainPPSOlapEsaAggregateBothMeasures
testSmartDbWrapExampleOlap
testSmartDbWrapExampleOlapMeasure
testYahooFinanceWrapExampleOlap


The problems seems to be:

* If I parse a logical query plan, for BaseCubeOp, I always use SparqlDerived. That one returns which measures? It returns only 
the explicit.

* If I return all, I have a problem with the Convert-Cube example query.

What do I want?

* For simplicity reasons Convert-Cube and Merge-Cubes automatically do a projection for the single explicit measure.

* BaseCubeOp should return all measures.

But now, what to change with Convert-Cube?


Example_QB_Datasets_QueryTest.java - *basic MDX examples work*
Example_QB_Datasets_DrillAcross_QueryTest.java *basic MDX drill-across examples ?*
Example_QB_Datasets_ConvertCube_QueryTest.java *MDX with correspondences examples ?*


LDCX_Performance_Evaluation_ConvertCubeTest.java


Problem: DerivedBaseCubeOp returns 8 measures but only 2 from the query.

Why do I need BaseCube at all?

* For Queries without Convert, I directly 

The actual problem is that BaseCube would not return the same amount of data than the schema suggests.

compile:

Problem:

* We may have Slice/Dice/etc. over BaseCube -> No problem.
* We may have Convert-Cube etc. over BaseCube -> No problem.
* We may have Drill-Across over BaseCube -> Problem.
* We may have Base-Cube directly -> Problem.

			// XXX: Apparently the nested-loop join does not work. This may be
			// because the headers of the outputs are different:

			/*
			 * [httplodgesisorglodpilotALLBUSvocabrdfgeo,
			 * httpontologycentralcom200901eurostatnsgeo,
			 * httpontologycentralcom200901eurostatnsindicna,
			 * httpontologycentralcom200901eurostatnsunit,
			 * httppurlorgdctermsdate,
			 * httppurlorglinkeddatasdmx2009measureobsValue]
			 * 
			 * 
			 * [httplodgesisorglodpilotALLBUSvocabrdfgeo0,
			 * httpontologycentralcom200901eurostatnsgeo0,
			 * httpontologycentralcom200901eurostatnsindicna0,
			 * httpontologycentralcom200901eurostatnsunit0,
			 * httppurlorgdctermsdate0,
			 * httppurlorglinkeddatasdmx2009measureobsValue_new]
			 */

==2014-07-25==
* What measures do we have?
** Implicit specific measures (with dataset)
** Implicity unspecific measures (without dataset)
** Explicit, unspecific measure

* Problem: Apparently, for hierarchies, we get several options. 
* Solution: Have test cases for metadata queries over drill-across cubes.

* Problems
** Every implicit aggregate measure is double
** Every hierarchies is double
** getMembers(Level:Measures) returns hierarchies, also?
** Apparently, we ask for other members if we ask for all members of the level measures: we query for a measure if...

Current setting of LDCX
* Crawling: no seeAlso, no members, now owl:sameAs, no subpropertyOf
* Integrity constraints: 


==2014-07-21==
* Status at submission.
* Next steps:
** Test-Case for automatic materialisation of GDP per Capita scenario: executeDrillAcrossGdpCap_AutomaticConvert 
in Example_QB_Datasets_ConvertCube_QueryTest.

==2014-07-17==
Problem: SPARQL construct query is not properly created, as is the N3 rule.

If we create the triple patterns for the measures. We once use the measures of inputcube1 (with more) and filter and once we use the already filtered measures which will result in a different measure number. 
	
I think, metadata should be created but not be used in querying if possible. *done*

Problem: Many duplicates in results of Convert-Cube operations. Problem is that we use shared dimensions
in query so that all possible combinations of shared dimension member combinations are used by newly
created observations.  

Solution: For any one combination should there not be only one possibility? No, since afterwards 
reasoning, again. Solution, remove reasoning after generation.

Problem: Apparently, the dimensions and measures are not properly propagated through the iterators.

Status?

* BaseCubeSparqlDerivedDatasetIterator propagates them all, but not in the data. <= Problem is: I drill-across it directly for comparison. I could also do a projection over it. If from MDX, I have always OLAP2SPARQL on top. 
* Drill-AcrossNestedLoop adds measures of two cubes
* Olap2SparqlIterator works with all
* ConvertSparqlIterator propagates all (from first cube), but only publishes info of non-implicit measures.

Drill-across(Merge-Cubes (Merge-Cubes (Convert-Cube (BaseCube (http://estatwrap.ontologycentral.com/id/nama_gdp_c#ds), (mio_eur2eur, {(http://ontologycentral.com/2009/01/eurostat/ns#unit,http://estatwrap.ontologycentral.com/dic/unit#MIO_EUR)},{},{(http://ontologycentral.com/2009/01/eurostat/ns#unit,http://estatwrap.ontologycentral.com/dic/unit#EUR)},{(1000000 * x)})), Convert-Cube (BaseCube (http://estatwrap.ontologycentral.com/id/nama_gdp_c#ds), (mio_eur2eur, {(http://ontologycentral.com/2009/01/eurostat/ns#unit,http://estatwrap.ontologycentral.com/dic/unit#MIO_EUR)},{},{(http://ontologycentral.com/2009/01/eurostat/ns#unit,http://estatwrap.ontologycentral.com/dic/unit#EUR)},{(1000000 * x)})), (computegdp, {(http://ontologycentral.com/2009/01/eurostat/ns#indic_na,http://estatwrap.ontologycentral.com/dic/indic_na#B1G)},{(http://ontologycentral.com/2009/01/eurostat/ns#indic_na,http://estatwrap.ontologycentral.com/dic/indic_na#D21_M_D31)},{(http://ontologycentral.com/2009/01/eurostat/ns#indic_na,http://estatwrap.ontologycentral.com/dic/indic_na#NGDP)},{(x1 + x2)})), BaseCube (http://estatwrap.ontologycentral.com/id/demo_pjan#ds), (computegdppercapita, {(http://ontologycentral.com/2009/01/eurostat/ns#indic_na,http://estatwrap.ontologycentral.com/dic/indic_na#NGDP)(http://ontologycentral.com/2009/01/eurostat/ns#unit,http://estatwrap.ontologycentral.com/dic/unit#EUR)},{(http://ontologycentral.com/2009/01/eurostat/ns#sex,http://estatwrap.ontologycentral.com/dic/sex#T)(http://ontologycentral.com/2009/01/eurostat/ns#age,http://estatwrap.ontologycentral.com/dic/age#TOTAL)},{(http://ontologycentral.com/2009/01/eurostat/ns#indic_na,http://estatwrap.ontologycentral.com/dic/indic_na#NGDPH)(http://ontologycentral.com/2009/01/eurostat/ns#unit,http://estatwrap.ontologycentral.com/dic/unit#EUR_HAB)},{(x1 / x2)})), Slice (BaseCube (http://estatwrap.ontologycentral.com/id/nama_aux_gph#ds), {}))

SPARQL CONSTRUCT query: PREFIX qb:<http://purl.org/linked-data/cube#> PREFIX olap4ld:<http://purl.org/olap4ld/>construct { _:outputcube <http://purl.org/linked-data/cube#dataSet> <http://localhost/deriveddataset/dataset1974913310/conversionfunction-2005164347> . 
<http://localhost/deriveddataset/dataset1974913310/conversionfunction-2005164347> <http://purl.org/linked-data/cube#structure> ?dsd1 . 
_:outputcube <http://ontologycentral.com/2009/01/eurostat/ns#indic_na> <http://estatwrap.ontologycentral.com/dic/indic_na#NGDPH> . 
_:outputcube <http://ontologycentral.com/2009/01/eurostat/ns#unit> <http://estatwrap.ontologycentral.com/dic/unit#EUR_HAB> . 
_:outputcube <http://lod.gesis.org/lodpilot/ALLBUS/vocab.rdf#geo> ?httplodgesisorglodpilotALLBUSvocabrdfgeo . 
_:outputcube <http://ontologycentral.com/2009/01/eurostat/ns#geo> ?httpontologycentralcom200901eurostatnsgeo . 
_:outputcube <http://purl.org/dc/terms/date> ?httppurlorgdctermsdate . 
_:outputcube <http://purl.org/linked-data/sdmx/2009/measure#obsValue> ?outputvalue1 . 
 } where { { 
 
 select  ?dsd1 ?dsd2  ((?input1value1 / ?input2value1) as ?outputvalue1)  ?httplodgesisorglodpilotALLBUSvocabrdfgeo  ?dsd2  ?httppurlorgdctermsdate  ?httpontologycentralcom200901eurostatnsgeo  ?input2value1  where {
 
 ?inputcube1 <http://purl.org/linked-data/cube#dataSet> <http://localhost/deriveddataset/dataset666074368/conversionfunction-270137444> . 
<http://localhost/deriveddataset/dataset666074368/conversionfunction-270137444> <http://purl.org/linked-data/cube#structure> ?dsd1 . 

?inputcube2 <http://purl.org/linked-data/cube#dataSet> <http://estatwrap.ontologycentral.com/id/demo_pjan#ds> . 
<http://estatwrap.ontologycentral.com/id/demo_pjan#ds> <http://purl.org/linked-data/cube#structure> ?dsd2 . 

?inputcube1 <http://ontologycentral.com/2009/01/eurostat/ns#indic_na> <http://estatwrap.ontologycentral.com/dic/indic_na#NGDP> . 
?inputcube1 <http://ontologycentral.com/2009/01/eurostat/ns#unit> <http://estatwrap.ontologycentral.com/dic/unit#EUR> . 

?inputcube2 <http://ontologycentral.com/2009/01/eurostat/ns#sex> <http://estatwrap.ontologycentral.com/dic/sex#T> . 
?inputcube2 <http://ontologycentral.com/2009/01/eurostat/ns#age> <http://estatwrap.ontologycentral.com/dic/age#TOTAL> . 

?inputcube1 <http://lod.gesis.org/lodpilot/ALLBUS/vocab.rdf#geo> ?httplodgesisorglodpilotALLBUSvocabrdfgeo . 
?inputcube2 <http://lod.gesis.org/lodpilot/ALLBUS/vocab.rdf#geo> ?httplodgesisorglodpilotALLBUSvocabrdfgeo . 
?inputcube1 <http://ontologycentral.com/2009/01/eurostat/ns#geo> ?httpontologycentralcom200901eurostatnsgeo . 
?inputcube2 <http://ontologycentral.com/2009/01/eurostat/ns#geo> ?httpontologycentralcom200901eurostatnsgeo . 

?inputcube1 <http://purl.org/dc/terms/date> ?httppurlorgdctermsdate . 
?inputcube2 <http://purl.org/dc/terms/date> ?httppurlorgdctermsdate . 
?inputcube1 <http://purl.org/linked-data/sdmx/2009/measure#obsValue> ?input1value1 . 
?inputcube2 <http://purl.org/linked-data/sdmx/2009/measure#obsValue> ?input2value1 . 

} } }
 


==2014-07-16==
* More nested Convert-Cube queries do not work, anymore.
* Maybe because of "isLoaded(dataset)"?
* Also, normalisation and reasoning is now done for each dataset.

==2014-07-14==
Problem: When I create the LogicalQueryPlan starting from a (preprocessed) dataset, I look for the "first" occurence of Base-Cube-Op and use that for the "schema".
Solution: Two possibilities:

1) Logical Olap Op can return the schema even though it is not executed, yet.

2) We really search for the first base-cube (on the left sides of the query plan).

Note later when "filtering" more logical query plans, we may need the logical ops to return the schema.

For now, we go just through them ourselves.

Now convert-cube is tested for isLoaded().

When creating "suitable" query plans, what do we need to check?

Let's see...

Problem: Apparently, Drill-Across results in empty cube. 


==2014-07-12==
Problem: Queries would not work, anymore. 
Solution: No dice, no dice.

Problem: Base-Cube would not forward the implicit measures. I should do that, though.
Solution: Base-Cube now forwards measures, but returned tuples are without (why, again?)

Problem: One drill-across query over example dataset:
Solution: We have to add the respective datasets with the links contained.
Status: Drill-Across: Errors 0, Failures 1

==2014-07-10==
Problem: Starting from the query over the global dataset, we currently do not properly "unfold" the query given the "available datasets". 
Solution: We should: Only drill-across over datasets that 1) show one of the queried measures, 2) does have all the "inquired dimensions" 3) dices only available dimensions.
Hint: This is quite important. The datasets in the FROM clause do NOT indicate a drill-across over the datasets, since the drill-across will most likely be empty. Instead, we are querying a global cube that can be build from those single cubes.

* Refactored building of Logical Query Plan.
* Now using "definition" of global cube, i.e., drill-across of all fitting datasets.

Problem: We should not concat header if all measures are the same. We should update metadata accordingly.
Solution: done

Problem: Again, what does the convert-cube operation do? I think, it removes the implicit measures.
Solution: As soon as we use convert-cube, we only work with the non-implicit measures. 

* Outer-join in case all measures are the same (is that really the case?)
* We should be querying over all datasets (scenario).
* Do I have a case where I only need information from a subset of datasets?
* E.g., if I query EU2020 with specific measures and take allbus into account. 

* Features that we need to "evaluate"
** Concrete measures (UNEMPLOY) <= Here, we only need information from a subset of datasets.
** Not concrete measures (UNEMPLOY)
** Concrete measures (EU2020 4/8)
** Compound measure (UNEMPLOY)

What do we have:

* EMPLOY with two datasets, specific measures
* EMPLOY with several datasets, specific measures <= use all datasets.
* EMPLOY with several datasets, unspecific measure <= shows "problem"
* EMPLOY with compound measure "Unemployment Fear Measure" <= try to solve "problem" and find Metric: 00, 2010, 0.82658; and 00, 1980, 0.92751235584 (as in ISEM paper!)

==2014-07-09==
* Changed "default" measures, see http://linked-data-cubes.org/index.php/EmbeddedSesameEngine#Default_Measure_Strategies
* Now NestedLoopJoin does Concat in case all measures are the same.
* We should make sure that in case all measures are the same, it suffices to have one value
from either cubes.
* Currently does not work, try again!

==2014-07-09==
* Finished refactoring of logical2physical operator plan:

===LogicalToPhysical Query Plan Strategy===
* See also: http://linked-data-cubes.org/index.php/EmbeddedSesameEngine#LogicalToPhysical_Query_Plan_Strategy
* Short: First used Vistor Pattern, now use Depth-first strategy

<pre>

==2014-07-07==

* Filtering for duplicates at getDimensions(), getHierarchies()... on all "unique" attributes,

now that we have duplication strategy.

* Problem: I have never combined OLAP-to-SPARQL with Convert-Cube/Merge-Cubes.

* Solution: Why can I not implement Convert-Cube/Merge-Cubes using SPARQL in an extended OLAP-to-SPARQL

algorithm? Since 1) Convert-Cube/Merge-Cubes represent "implicit" datasets that I may want to materialise

for faster query processing, 2) whereas Projection, Dice, Slice are issued by the user explicitly,

Convert-Cube / Merge-Cubes are run automatically by the system to increase the number of answers to

an OLAP query (OLAP-to-SPARQL query is fixed on one dataset, OLAP query issues drill-across over all

possible datasets, returning several answers to a query.

* To do OLAP-to-SPARQL over Convert-Cube/Merge-Cubes, I need to execute Convert-Cube/Merge-Cubes separately

and only feed the data cube URI.

Problem:

What operators do we have:

BaseCube

Projection

Dice

Slice

Roll-Up

Drill-Across

What iterators do we have:

GetCubesSparqlIterator <= Not implemented - at all.

OlapDrillAcross2JoinSesameVisitor

DrillAcrossNestedLoopJoinSesameIterator <= works over every iterator that returns tuples: Olap2SparqlAlgorithmSesameIterator (OLAP-to-SPARQL), Convert-Cube (ConvertSparqlDerivedDatasetIterator), Base-Cube (BaseCubeSparqlDerivedDatasetIterator). Requires 2 input iterators (that return tuples).

Iterator x Iterator -> Tuples

For query trees without Drill-Across and Convert-Cube

* Olap2SparqlSesameVisitor

Olap2SparqlAlgorithmSesameIterator <= OLAP-to-SPARQL algorithm. Works only over engine that provides SPARQL functionality + No input iterators.

Engine -> Tuples

For query trees with Convert-Cube / Base-Cube

* Olap2SparqlSesameDerivedDatasetVisitor

BaseCubeSparqlDerivedDatasetIterator <= Works, but simply returns tuple representation (since data is already loaded by then from: calling getCubes on EmbeddedSesameEngine). Requires repository + No input iterators.

(Repo) -> Tuples (Repo)

ConvertSparqlDerivedDatasetIterator <= Convert-Cube implementation. Requires repository + 1-2 input iterators.

Iterator x Iterator (Repo) -> Tuples (Repo)

SliceSparqlDerivedDatasetIterator <= Not implemented, yet.

DrillAcrossSparqlDerivedDatasetIterator <= Not implemented, yet: Two datasets are joined. For now, just return the first

Solution:

* Make BaseCube only gets QB dataset URI *todo still, errors*

* Make BaseCubeSparqlDerivedDatasetIterator to only get QB dataset URI

(Engine) -> Tuple (Engine)

* Make Olap2SparqlAlgorithmSesameIterator to take in engine and iterator for cube name.

Iterator (Engine) -> Tuple (Engine)

* Leave ConvertSparqlDerivedDatasetIterator

Iterator x Iterator (Engine) -> Tuples (Engine)

* Reduce number of visitors?

** Only works with LQPs that have 1) any number of Drill-Across at the beginning (for joining, comparison, merging with DrillAcrossNestedLoopJoinSesameIterator), 2) one set of Projection, Dice, Slice, Roll-Up (for OLAP operations with Olap2SparqlAlgorithmSesameIterator), 3) any number of Convert-Cube / Merge-Cubes (for ConvertSparqlDerivedDatasetIterator), and 4) any number of Base-Cube (for BaseCubeSparqlDerivedDatasetIterator).

** Convert-Cube / Merge-Cubes / Base-Cube may be added automatically.

Then, we can have DrillAcrossNestedLoopJoinSesameIterator -> Olap2SparqlAlgorithmSesameIterator -> ConvertSparqlDerivedDatasetIterator -> BaseCubeSparqlDerivedDatasetIterator.

Before, only ConvertSparqlDerivedDatasetIterator -> BaseCubeSparqlDerivedDatasetIterator OR ConvertSparqlDerivedDatasetIterator -> ConvertSparqlDerivedDatasetIterator OR DrillAcrossNestedLoopJoinSesameIterator on top of everything (yes, Drill-Across worked for both kinds of trees) was possible.

==Problem: How to build the Visitor? How to create Phyical Iterator Plan?==

* How will the Logical Query Plan look like? 1) any number of Drill-Across at the beginning (for joining, comparison, merging with DrillAcrossNestedLoopJoinSesameIterator), 2) one set of Projection, Dice, Slice, Roll-Up (for OLAP operations with Olap2SparqlAlgorithmSesameIterator), 3) any number of Convert-Cube / Merge-Cubes (for ConvertSparqlDerivedDatasetIterator), and 4) any number of Base-Cube (for BaseCubeSparqlDerivedDatasetIterator).

** Convert-Cube / Merge-Cubes / Base-Cube may be added automatically.

* How about:

** Visitor Pattern

** If Drill-Across: Recursive Application of new Visitor Pattern, taking in two Iterators that are connected in a new Iterator (DrillAcrossNestedLoopJoinSesameIterator)

** If Projection, Dice, Slice, Roll-Up: Collection of information from these operators.

** If Base-Cube: Using dataseturi and connecting a new BaseCubeSparqlDerivedDatasetIterator to one or more ConvertSparqlDerivedDatasetIterator Olap2SparqlAlgorithmSesameIterator

** If Convert-Cube:

Collection of information from these operators.

One possibility: Go through tree. For Convert-Cube / Base-Cube use Olap2SparqlSesameDerivedDatasetVisitor and for all else use Olap2SparqlSesameVisitor and each result add to iterator list that then will be drilled-across with own Iterator.

One could keep that assuming that after Convert-Cube and Base-Cube only Convert-Cube or Base-Cube can show up.

However, what about Olap2SparqlSesameVisitor? For Convert-Cube / Base-Cube, one could use Olap2SparqlSesameDerivedDatasetVisitor and the result will then be used as input to the OLAP-to-SPARQL iterator.

Try it!

Problem: Iterators need SPARQL functionality from the LDE. *implemented, refactored*

Can I go depth-first through the logical query plan to "build" the physical query plan?

Pseudo-code:

compile(Node node) {

Iterator theIterator;

if (node instanceof Drill-Across) {

    //recursively go through children

    Iterator iterator1 = compile(node.getInput1());

    Iterator iterator2 = compile(node.getInput2());

    theIterator = new Drill-Across-Iterator(iterator1, iterator2);

}

if (node instanceof Roll-Up, Slice, Dice, Projection) {

    //recursively go through cildren

    store data about Roll-Up, Slice, Dice, Projection;

    if (node.getInput() instanceof Base-Cube/Convert-Cube) {

        Iterator iterator1 = compile(node.getInput());

        theIterator = new OLAP-to-SPARQL(iterator1)

    } else {

        theIterator1 = compile(node.getInput());

    }

}

if (node instanceof Convert-Cube) {

    //recursively

    Iterator iterator1 = compile(node.getInput1());

    Iterator iterator2 = compile(node.getInput2());

    theIterator = new Convert-Cube-Iterator(iterator1, iterator2);

}

if (node instanceof Base-Cube) {

    theIterator = new Base-Cube-Iterator(dataseturi)

}

return theIterator;

}

</pre>


==2014-07-08==
* Refactored operators and interators to allow combination of OLAP-to-SPARQL and Convert-Cube.
* See https://intra.b-kaempgen.de/dawiki/index.php/Resubmission_of_Convert-Cube_Paper#Is_there_an_.22equivalent.22_description_of_the_query_in_ISEM_paper.3F
* Status: Examples: Failures 10.

==2014-07-07==
* Filtering for duplicates at getDimensions(), getHierarchies()... on all "unique" attributes,
now that we have duplication strategy.

* Problem: I have never combined OLAP-to-SPARQL with Convert-Cube/Merge-Cubes. 
* Solution: Why can I not implement Convert-Cube/Merge-Cubes using SPARQL in an extended OLAP-to-SPARQL 
algorithm? Since 1) Convert-Cube/Merge-Cubes represent "implicit" datasets that I may want to materialise 
for faster query processing, 2) whereas Projection, Dice, Slice are issued by the user explicitly,
Convert-Cube / Merge-Cubes are run automatically by the system to increase the number of answers to 
an OLAP query (OLAP-to-SPARQL query is fixed on one dataset, OLAP query issues drill-across over all 
possible datasets, returning several answers to a query. 
* To do OLAP-to-SPARQL over Convert-Cube/Merge-Cubes, I need to execute Convert-Cube/Merge-Cubes separately
and only feed the data cube URI.

Problem: 

What operators do we have:

BaseCube
Projection
Dice
Slice
Roll-Up
Drill-Across

What iterators do we have:

GetCubesSparqlIterator <= Not implemented - at all.

OlapDrillAcross2JoinSesameVisitor

DrillAcrossNestedLoopJoinSesameIterator <= works over every iterator that returns tuples: Olap2SparqlAlgorithmSesameIterator (OLAP-to-SPARQL), Convert-Cube (ConvertSparqlDerivedDatasetIterator), Base-Cube (BaseCubeSparqlDerivedDatasetIterator). Requires 2 input iterators (that return tuples).

Iterator x Iterator -> Tuples 

For query trees without Drill-Across and Convert-Cube

* Olap2SparqlSesameVisitor

Olap2SparqlAlgorithmSesameIterator <= OLAP-to-SPARQL algorithm. Works only over engine that provides SPARQL functionality + No input iterators.

Engine -> Tuples

For query trees with Convert-Cube / Base-Cube

* Olap2SparqlSesameDerivedDatasetVisitor 

BaseCubeSparqlDerivedDatasetIterator <= Works, but simply returns tuple representation (since data is already loaded by then from: calling getCubes on EmbeddedSesameEngine). Requires repository + No input iterators.

(Repo) -> Tuples (Repo)

ConvertSparqlDerivedDatasetIterator <= Convert-Cube implementation. Requires repository + 1-2 input iterators.

Iterator x Iterator (Repo) -> Tuples (Repo)

SliceSparqlDerivedDatasetIterator <= Not implemented, yet.

DrillAcrossSparqlDerivedDatasetIterator <= Not implemented, yet: Two datasets are joined. For now, just return the first

Solution: 

* Make BaseCube only gets QB dataset URI

* Make BaseCubeSparqlDerivedDatasetIterator to only get QB dataset URI

(Repo) -> Tuple (Repo)

* Make Olap2SparqlAlgorithmSesameIterator to take in repository

Iterator (Repo) -> Tuple (Repo)

* Leave ConvertSparqlDerivedDatasetIterator 

Iterator x Iterator (Repo) -> Tuples (Repo)

* Reduce number of visitors? 
** Only works with LQPs that have 1) any number of Drill-Across at the beginning (for joining, comparison, merging with DrillAcrossNestedLoopJoinSesameIterator), 2) one set of Projection, Dice, Slice, Roll-Up (for OLAP operations with Olap2SparqlAlgorithmSesameIterator), 3) any number of Convert-Cube / Merge-Cubes (for ConvertSparqlDerivedDatasetIterator), and 4) any number of Base-Cube (for BaseCubeSparqlDerivedDatasetIterator).
** Convert-Cube / Merge-Cubes / Base-Cube may be added automatically. 

Then, we can have DrillAcrossNestedLoopJoinSesameIterator -> Olap2SparqlAlgorithmSesameIterator -> ConvertSparqlDerivedDatasetIterator -> BaseCubeSparqlDerivedDatasetIterator.

Before, only ConvertSparqlDerivedDatasetIterator -> BaseCubeSparqlDerivedDatasetIterator OR ConvertSparqlDerivedDatasetIterator -> ConvertSparqlDerivedDatasetIterator OR DrillAcrossNestedLoopJoinSesameIterator on top of everything (yes, Drill-Across worked for both kinds of trees) was possible.


==2014-07-06==
XXX: Does it work without global cube definition?

Yes. Seems for single cube test cases.

? for multi cube test cases.

Problem: The system is looking for the multicube which is not existing.

Solution: We manage a list of cubes, instead.

* Remove global cube definition = define queried datasets (14:30)

Solution:

* Olap4ldCellSetMetadata: list of cubes?

* Olap4ldCellSet.createLogicalQueryPlan: list of cubes?

* Olap4ldCellSet -> instead of the one single cube, the correct cube.

* Olap4ldCellSet.createMetadata -> instead of metaData.cube.getMeasures, we simply take the first measure of the first cube.

* CellSetMetadata.getCube() -> a CellSet has exactly one cube. That was the reason for creating the "Global cube". This is because we implicitly define a "global cube" with all dimensions and members. 

Final solution:

No, we have to return "global cube", since this is how MDX works. The question is how the global
cube is defined and how the logical query plan is created.  

? for ldcx visualisation.

Let us try whether our new reasoning strategy can be visualised properly.

*no f** idea since I cannot debug and apparently the MDX (generateMDX) is not done properly.*

XXX: Does it work with all Drill-Across queries so that we can now have Evaluation?

XXX: Can I now have one SPARQL query? Can I define behaviour if measures are the same (for convert-cube?)

XXX: Extension to Compound measures?

XXX: Extension to Convert Cube / Merge Cubes?

XXX: Demo with free MDX field?

==2014-07-05==
Working on Drill-Across: Entity consolidation: 
* In Weller 2012 we do traditional entity consolidation with HashSets in a HashMap.
* In ISEM, we created items list mapping from resource to canonical. Then, going through sameAses, I create closure from dimension to canonical. Then, for each canonical I create exactly one dimension/member.
* In Drill-Across, 
** getCanonical for a node
** replaceIdentifiersWithCanonical(List<Node[]> result)
** XXX: I also need to remove duplicates
** After Olap2SparqlAlgorithm is executed, I replace identifiers (thus, Drill-Across does not need to be equivalence aware)

* What shall I do?
** Andreas recommended to do proper entity consolidation over the RDF.
** Or do "duplication", but this would result in duplicate results of "dimensions".
** Or use reasoning capability of Sesame.

* XXX: What about this "damn" global elements? Do I really need them? 

1) Let's try:

* Remove global elements.

* Remove replaceIdentifiersWithCanonical

* Setup Sesame to do reasoning? Does not work. I have no idea how Sesame reacts on what reasoning statements. 

Example_QB_Datasets_Query_Test.java: 28/28, Errors 3, Failures 12

2) Let's try

* Duplication strategy / owl:sameAs equivalence rules from http://www.w3.org/TR/owl2-profiles/ and http://semanticweb.org/OWLLD/

Problem: Failed specification check: IC-19. If a dimension property has a qb:codeList, then the value of the dimension property on every qb:Observation must be in the code list.  <br/>

Solution: Probably because the restriction would need to be different in case several codeLists are existing? Or even if degenerate dimension, also. For now, we simply disable IC.

Problem: LQP does not look good. 

Drill-across(Rollup (Slice (Dice (Projection (BaseCube (http://estatwrap.ontologycentral.com/id/tec00115#ds), {http://purl.org/linked-data/sdmx/2009/measure#obsValuehttp://estatwrap.ontologycentral.com/id/tec00115#dsAGGFUNCAVG, http://purl.org/linked-data/sdmx/2009/measure#obsValuehttp://estatwrap.ontologycentral.com/id/tec00115#dsAGGFUNCAVG}), (http://lod.gesis.org/lodpilot/ALLBUS/geo.rdf#list = http://lod.gesis.org/lodpilot/ALLBUS/geo.rdf#00)), {http://lod.gesis.org/lodpilot/ALLBUS/geo.rdf#list, http://ontologycentral.com/2009/01/eurostat/ns#geo, http://ontologycentral.com/2009/01/eurostat/ns#unit}), {}), Rollup (Slice (Dice (Projection (BaseCube (http://lod.gesis.org/lodpilot/ALLBUS/ZA4570v590.rdf#ds), {http://purl.org/linked-data/sdmx/2009/measure#obsValuehttp://lod.gesis.org/lodpilot/ALLBUS/ZA4570v590.rdf#dsAGGFUNCAVG}), (http://lod.gesis.org/lodpilot/ALLBUS/geo.rdf#list = http://lod.gesis.org/lodpilot/ALLBUS/geo.rdf#00)), {http://lod.gesis.org/lodpilot/ALLBUS/geo.rdf#list, http://ontologycentral.com/2009/01/eurostat/ns#geo, http://lod.gesis.org/lodpilot/ALLBUS/vocab.rdf#variable}), {}))

Solution: Maybe we need to remove "duplicates" explicitly? - Maybe distinct?

Solution: It was wrong to say:

<http://lod.gesis.org/lodpilot/ALLBUS/geo.rdf#list> <http://www.w3.org/2002/07/owl#sameAs> <http://ontologycentral.com/2009/01/eurostat/ns#geo>.

We now say:

<http://lod.gesis.org/lodpilot/ALLBUS/geo.rdf#list> <http://www.w3.org/2002/07/owl#sameAs> <http://rdfdata.eionet.europa.eu/ramon/ontology/NUTSRegion>.

Also, dimension is now never used as a name for the hierararchy, but we use the range for the hierarchy. Same for the level.

Question: Do we then have a valid query?

SELECT
{[httpXXX3AXXX2FXXX2FpurlYYYorgXXX2FlinkedZZZdataXXX2FsdmxXXX2F2009XXX2FmeasureXXX23obsValuehttpXXX3AXXX2FXXX2FlodYYYgesisYYYorgXXX2FlodpilotXXX2FALLBUSXXX2FZA4570v590YYYrdfXXX23dsAGGFUNCAVG], [httpXXX3AXXX2FXXX2FpurlYYYorgXXX2FlinkedZZZdataXXX2FsdmxXXX2F2009XXX2FmeasureXXX23obsValuehttpXXX3AXXX2FXXX2FestatwrapYYYontologycentralYYYcomXXX2FidXXX2Ftec00115XXX23dsAGGFUNCAVG]} ON COLUMNS,
CrossJoin(Members([httpXXX3AXXX2FXXX2FpurlYYYorgXXX2FdcXXX2FtermsXXX2Fdate]), Members([httpXXX3AXXX2FXXX2FlodYYYgesisYYYorgXXX2FlodpilotXXX2FALLBUSXXX2FvocabYYYrdfXXX23geo])) ON ROWS
FROM [httpXXX3AXXX2FXXX2FestatwrapYYYontologycentralYYYcomXXX2FidXXX2Ftec00115XXX23dsXXX2ChttpXXX3AXXX2FXXX2FlodYYYgesisYYYorgXXX2FlodpilotXXX2FALLBUSXXX2FZA4570v590YYYrdfXXX23ds]
WHERE {[httpXXX3AXXX2FXXX2FlodYYYgesisYYYorgXXX2FlodpilotXXX2FALLBUSXXX2FgeoYYYrdfXXX2300]}

Works, but:

Measures were duplicated (probably because of owl:sameAs reasoning), thus we had to switch on "distinct" in query.

Similarly, in the output, we have several members, probably because they are also returned several times. Thus, also distinct? Hogan already stated that duplicate results may confuse people.

XXX Also, is result correct?

|  |      |  |  00      |      461.33 |         1.1 |

|      461.33 |         1.1 |",

Yes!

XXX: Does it also work with example datasets?

Example_QB_Datasets_Query_Test.java: 28/28, Errors 4, Failures 8

* Problem with Eurostat: Apparently allbus geo needs to be used (why? maybe because not complete reasoning)

httpXXX3AXXX2FXXX2FlodYYYgesisYYYorgXXX2FlodpilotXXX2FALLBUSXXX2FvocabYYYrdfXXX23geo

* Problem with ical:dtend. Apparently we cannot use it directly as hierarchy name since it is a blank node.

Solution: We can still filter for it if we only filter for strings in all our queries.

XXX: Can I also remove replacement of canonical?

Problem: node18s3pb4llx1221 some hierarchies are blanknodes

Solution: Check on "blank node" before filtering.

XXX: Does it work without global cube definition?

XXX: Does it work with all Drill-Across queries so that we can now have Evaluation?


==2014-05-11/12/13==
* Finished experiments.

==2014-05-09==
* Changed Sail to Repository.

==2014-05-08==
* Convert-Cube / Merge-Cubes implemented.
* Problem: For Drill-Across, we have added several measures. As name, we use the measure property, the dataset URI and the aggregation function string. However, the select query contains a triple pattern for the Measures dimension and triple patterns for those additional measures that we have created. 
* Solution: getString() had to be used!
* Problem: The physical query plan is executed, immediately? Can we not return it first? 
* Now all iterators only fire if needed (init() also implemented)
* Remaining: Weakness with current execution. If below convert cube is a query tree, if I call getXXX(), I might get bogus. If I did it right, I would make sure that with getXXX() either iterator is executed or at least made sure that correct metadata is returned.

==2014-05-07==
* Problem: So far we only supported one Drill-Across per query. 
* Solution: Now, the Drill-Across visitor goes through the entire tree and picks all non-Drill-Across subtrees converted to Iterator with OLAP-to-SPARQL.

* Problem: Results are empty.
* Solution: For nested-loop join, we need to be able to init the Drill-Across iterator, which first was not implemented.

==2014-05-04==
* Removed all compliation problems due to refactoring.
* Correctly computed comparison of Employment Fear vs. Real GDP Growth Rate.
* Growth rate:
** http://epp.eurostat.ec.europa.eu/tgm/table.do?tab=table&init=1&plugin=1&language=en&pcode=tec00115 (not tsieb020, anymore, renamed? See number of triples)
** http://estatwrap.ontologycentral.com/data/tec00115
* Integration/Entity consolidation of gesis:geo and estatwrap:geo as well as gesis-geo:00 and estatwrap-geo:DE included.
* No complex measure included.

==2014-05-03==
* Lots of refactoring (Restrictions are now Nodes)
* Worked on entity consolidation...
* Problem: If we compare own Nodes with Nodes from metadata SPARQL queries, we can never be sure when it is a Resource and when it is a Literal. In some case we sometimes get returned Literal and sometimes Resources (e.g., for measures).
Solution: In those cases, we compare the toString results.
* See: http://linked-data-cubes.org/index.php/EmbeddedSesameEngine#Entity_Consolitation
4-05-02==
* No success with entity consolidation for Drill-Across.
* Problem: If I just replace all occurrences of a URI with the canonical form, I have the problem 
that Estatwrap is using the same URI for Dim / Hier / Level, whereas GESIS uses different ones for 
Dim / Hier / Level. This is because Estatwrap uses rdfs:range nuts:Region whereas GESIS uses a code 
list #list.
* What do we want? 
** Canonical identifier for dimension? gesis:geo (selected because first dataset)
** Canonical identifier for hierarchy? gesis:geo (since estatwrap hierarchy is the same as estatwrap dimension) - However, 
** 
* How about defining that #list is also owl:sameAs estatwrap:geo? In this case, 

Todo: 
A first step would be to "consolidate" dimensions and members, i.e., to only return canonical identifiers.

A second step would be to only use those canonical identifiers (let's make it the first occurence) in MDX queries (so that those identifiers can be found).

A third step would be to change the generation of OLAP operators to use the right Linked Data identifiers for the respective dataset. However, how to know the correct identifier? 

A forth step would be to change the Drill-Across operator to consider equivalences.

===Convert-Cube===
* Now I use inputcube1, inputcube2 and outputcube.
* Also, I have written all reconciliation correspondences.
* Next steps:
* Implement Merge-Cubes
* Compare GDP per capita computed and GDP per capita directly.
* Extend the case study with external datasets

===Entity consolidation===
SELECT {[httpXXX3AXXX2FXXX2FpurlYYYorgXXX2FlinkedZZZdataXXX2FsdmxXXX2F2009XXX2FmeasureXXX23obsValueAGGFUNCAVG], [httpXXX3AXXX2FXXX2FpurlYYYorgXXX2FlinkedZZZdataXXX2FsdmxXXX2F2009XXX2FmeasureXXX23obsValuehttpXXX3AXXX2FXXX2FlodYYYgesisYYYorgXXX2FlodpilotXXX2FALLBUSXXX2FZA4570v590YYYrdfXXX23dsAGGFUNCSUM], [httpXXX3AXXX2FXXX2FpurlYYYorgXXX2FlinkedZZZdataXXX2FsdmxXXX2F2009XXX2FmeasureXXX23obsValuehttpXXX3AXXX2FXXX2Folap4ldYYYgooglecodeYYYcomXXX2FgitXXX2FOLAP4LDZZZtrunkXXX2FtestsXXX2FestatwrapXXX2Ftec00114_dsYYYrdfXXX23dsAGGFUNCSUM]} ON COLUMNS, 

{Members([httpXXX3AXXX2FXXX2FpurlYYYorgXXX2FdcXXX2FtermsXXX2Fdate])} ON ROWS 

FROM [httpXXX3AXXX2FXXX2Folap4ldYYYgooglecodeYYYcomXXX2FgitXXX2FOLAP4LDZZZtrunkXXX2FtestsXXX2FestatwrapXXX2Ftec00114_dsYYYrdfXXX23dsXXX2ChttpXXX3AXXX2FXXX2FlodYYYgesisYYYorgXXX2FlodpilotXXX2FALLBUSXXX2FZA4570v590YYYrdfXXX23ds] 

WHERE CrossJoin({[httpXXX3AXXX2FXXX2FlodYYYgesisYYYorgXXX2FlodpilotXXX2FALLBUSXXX2FgeoYYYrdfXXX2300], [httpXXX3AXXX2FXXX2Folap4ldYYYgooglecodeYYYcomXXX2FdicXXX2FgeoXXX23DE]},{[httpXXX3AXXX2FXXX2FlodYYYgesisYYYorgXXX2FlodpilotXXX2FALLBUSXXX2FvariableYYYrdfXXX23v590_1]})


Logical query plan: Drill-across(

Rollup (Slice (Dice (Projection (BaseCube (http://olap4ld.googlecode.com/git/OLAP4LD-trunk/tests/estatwrap/tec00114_ds.rdf#ds), {http://purl.org/linked-data/sdmx/2009/measure#obsValueAGGFUNCAVG, http://purl.org/linked-data/sdmx/2009/measure#obsValuehttp://olap4ld.googlecode.com/git/OLAP4LD-trunk/tests/estatwrap/tec00114_ds.rdf#dsAGGFUNCSUM}), (http://lod.gesis.org/lodpilot/ALLBUS/geo.rdf#list = http://lod.gesis.org/lodpilot/ALLBUS/geo.rdf#00 AND http://lod.gesis.org/lodpilot/ALLBUS/variable.rdf#list = http://lod.gesis.org/lodpilot/ALLBUS/variable.rdf#v590_1) OR (http://lod.gesis.org/lodpilot/ALLBUS/geo.rdf#list = http://olap4ld.googlecode.com/dic/geo#DE AND http://lod.gesis.org/lodpilot/ALLBUS/variable.rdf#list = http://lod.gesis.org/lodpilot/ALLBUS/variable.rdf#v590_1)), {http://ontologycentral.com/2009/01/eurostat/ns#aggreg95, http://ontologycentral.com/2009/01/eurostat/ns#geo, http://ontologycentral.com/2009/01/eurostat/ns#indic_na}), {}), 


Rollup (Slice (Dice (Projection (BaseCube (http://lod.gesis.org/lodpilot/ALLBUS/ZA4570v590.rdf#ds), {http://purl.org/linked-data/sdmx/2009/measure#obsValueAGGFUNCAVG, http://purl.org/linked-data/sdmx/2009/measure#obsValuehttp://lod.gesis.org/lodpilot/ALLBUS/ZA4570v590.rdf#dsAGGFUNCSUM}), (http://lod.gesis.org/lodpilot/ALLBUS/geo.rdf#list = http://lod.gesis.org/lodpilot/ALLBUS/geo.rdf#00 AND http://lod.gesis.org/lodpilot/ALLBUS/variable.rdf#list = http://lod.gesis.org/lodpilot/ALLBUS/variable.rdf#v590_1) OR (http://lod.gesis.org/lodpilot/ALLBUS/geo.rdf#list = http://olap4ld.googlecode.com/dic/geo#DE AND http://lod.gesis.org/lodpilot/ALLBUS/variable.rdf#list = http://lod.gesis.org/lodpilot/ALLBUS/variable.rdf#v590_1)), {http://lod.gesis.org/lodpilot/ALLBUS/vocab.rdf#geo, http://lod.gesis.org/lodpilot/ALLBUS/vocab.rdf#variable}), {}))

What problems do we see here?

Assume we really set estatwrap:geo owl:sameAs gesis:geo. This is what we do here: http://people.aifb.kit.edu/bka/Public/cube_additionalRDF.rdf

Then, both dimensions may have the same two codelists or define different ranges, e.g., nuts:region.

In ISEM paper, we 1) use canonical identifiers for dimensions and members when storing the metadata and 2) from the multiCubeDimensionsClosureTable mapping canonical values to dimension URIs use all dimension URIs in a OR filter clause for querying the data (which of course makes the query for data quite slow). 

However, something similar, we could do also: 

1) When querying for dimensions / hierarchies / levels and members I consolidate and use a canonical identifier which I map to original dimension URIs. Thus, we will have as identifiers new canonical ones. XXX: Could we not instead allow all possible identifiers?

2) When querying for facts I use all dimension URIs in an OR filter clause and all member URIs in another OR filter clause.

Why does our current query not work? Since we filter for gesis:geo although for estatwrap we would need to filter for estatwrap:geo. 

There are different possibilities:

* We could leave the OLAP-to-SPARQL algorithm as it is and make sure that the proper dimensions are used as input to it. However, we would need to extend Drill-Across to consider equal dimensions and members.
* We could extend the OLAP-to-SPARQL algorithm to query for all possible dimensions or members.

A first step would be to "consolidate" dimensions and members, i.e., to only return canonical identifiers.

A second step would be to only use those canonical identifiers (let's make it the first occurence) in MDX queries (so that those identifiers can be found).

A third step would be to change the generation of OLAP operators to use the right Linked Data identifiers for the respective dataset. However, how to know the correct identifier? 

A forth step would be to change the Drill-Across operator to consider equivalences.

How have I done it for ISEM paper? I created canonical identifiers for metadata and queried simply for every possible URI in data.

What is different with triple stores? I have a query on the global cube with canonical identifiers. When I create the initial query plan, I use elements from the single datasets. This works in case the elements are directly represented in the single datasets. However, in case of canonical identifiers, I do not directly have the elements represented with this name. One thing I could do is to use those canonical identifiers still, but internally map.

Problem: If I just replace all occurrences of a URI with the canonical form, I have the problem that Estatwrap is using the same URI for Dim / Hier / Level, whereas GESIS uses different ones for Dim / Hier / Level. This is because Estatrap uses rdfs:range nuts:Region whereas GESIS uses a code list #list.

How was that in ISEM paper? In ISEM paper, I did not represent Hierarchies, yet. I assumed every dimension to have exactly one hierarchy and level with the same name as the dimension (see findDimensionHierarchies). 

==2014-05-01==

===First, integrate properly GESIS and Estatwrap===
Drill-across(

Rollup (Slice (Dice (Projection (BaseCube (http://olap4ld.googlecode.com/git/OLAP4LD-trunk/tests/estatwrap/tec00114_ds.rdf#ds), {http://purl.org/linked-data/sdmx/2009/measure#obsValueAGGFUNCAVG, http://purl.org/linked-data/sdmx/2009/measure#obsValuehttp://olap4ld.googlecode.com/git/OLAP4LD-trunk/tests/estatwrap/tec00114_ds.rdf#dsAGGFUNCSUM}), (http://lod.gesis.org/lodpilot/ALLBUS/geo.rdf#list = http://lod.gesis.org/lodpilot/ALLBUS/geo.rdf#00) OR (http://lod.gesis.org/lodpilot/ALLBUS/geo.rdf#list = http://olap4ld.googlecode.com/dic/geo#DE)), {http://ontologycentral.com/2009/01/eurostat/ns#aggreg95, http://ontologycentral.com/2009/01/eurostat/ns#geo, http://ontologycentral.com/2009/01/eurostat/ns#indic_na}), {}), 


Rollup (Slice (Dice (Projection (BaseCube (http://lod.gesis.org/lodpilot/ALLBUS/ZA4570v590.rdf#ds), {http://purl.org/linked-data/sdmx/2009/measure#obsValueAGGFUNCAVG, http://purl.org/linked-data/sdmx/2009/measure#obsValuehttp://lod.gesis.org/lodpilot/ALLBUS/ZA4570v590.rdf#dsAGGFUNCSUM}), (http://lod.gesis.org/lodpilot/ALLBUS/geo.rdf#list = http://lod.gesis.org/lodpilot/ALLBUS/geo.rdf#00) OR (http://lod.gesis.org/lodpilot/ALLBUS/geo.rdf#list = http://olap4ld.googlecode.com/dic/geo#DE)), {http://lod.gesis.org/lodpilot/ALLBUS/vocab.rdf#geo, http://lod.gesis.org/lodpilot/ALLBUS/vocab.rdf#variable}), {}))

... Now, we see that we need to start integrate the datasets. How to do that, entity consolidation 
as in ISEM paper?

What exactly do we need to do? 

===Dice for integration===
The problem seems that we dice only for the gesis:geo dimension.

First, check whether dice works at all:

SELECT {[httpXXX3AXXX2FXXX2FpurlYYYorgXXX2FlinkedZZZdataXXX2FsdmxXXX2F2009XXX2FmeasureXXX23obsValueAGGFUNCAVG], [httpXXX3AXXX2FXXX2FpurlYYYorgXXX2FlinkedZZZdataXXX2FsdmxXXX2F2009XXX2FmeasureXXX23obsValuehttpXXX3AXXX2FXXX2FlodYYYgesisYYYorgXXX2FlodpilotXXX2FALLBUSXXX2FZA4570v590YYYrdfXXX23dsAGGFUNCSUM]} 
ON COLUMNS, 

{Members([httpXXX3AXXX2FXXX2FpurlYYYorgXXX2FdcXXX2FtermsXXX2Fdate])} ON ROWS 

FROM [httpXXX3AXXX2FXXX2FlodYYYgesisYYYorgXXX2FlodpilotXXX2FALLBUSXXX2FZA4570v590YYYrdfXXX23ds]

WHERE CrossJoin({[httpXXX3AXXX2FXXX2FlodYYYgesisYYYorgXXX2FlodpilotXXX2FALLBUSXXX2FgeoYYYrdfXXX2300]},{[httpXXX3AXXX2FXXX2FlodYYYgesisYYYorgXXX2FlodpilotXXX2FALLBUSXXX2FvariableYYYrdfXXX23v590_1]})

Note, we are not allowed to ask (e.g., dice) for non-existing members.

Question: How is dice defined?

According to OLAP4LD demo paper, Dice removes for every possible member combination on axes filtered dimension members. This probably is not completely true. We dice as specified in WHERE clause (filter axis). A filter axis would return member combinations (positions) from a fixed set of levels (or hierarchies) from the filter axis. Thus, for filter axis (just as for the column and row axis), we create a list of member combinations (positions). In filter axis, either we have a list of members, or we have a list of lists. We want to have returned a list of positions. Every position is a possible combination of members of a fixed set of levels. 

In case there are members, directly, we return 

Execute logical query plan: Rollup (Slice (Dice (Projection (BaseCube (http://lod.gesis.org/lodpilot/ALLBUS/ZA4570v590.rdf#ds), {http://purl.org/linked-data/sdmx/2009/measure#obsValueAGGFUNCAVG, http://purl.org/linked-data/sdmx/2009/measure#obsValuehttp://lod.gesis.org/lodpilot/ALLBUS/ZA4570v590.rdf#dsAGGFUNCSUM}), (http://lod.gesis.org/lodpilot/ALLBUS/geo.rdf#list = http://lod.gesis.org/lodpilot/ALLBUS/geo.rdf#00) OR (http://lod.gesis.org/lodpilot/ALLBUS/geo.rdf#list = http://lod.gesis.org/lodpilot/ALLBUS/variable.rdf#v590_0)), {http://lod.gesis.org/lodpilot/ALLBUS/vocab.rdf#geo, http://lod.gesis.org/lodpilot/ALLBUS/vocab.rdf#variable}), {})

This seemingly "error" to use geo for both OR sides is ok since every OR should have the same "signature".

Watch out: Saiku would use CrossJoin for representing AND: CrossJoin({[httpXXX3AXXX2FXXX2FpublicYYYbZZZkaempgenYYYdeXXX3A8080XXX2FcikXXX2F1141391XXX23id]}, {[httpXXX3AXXX2FXXX2FpublicYYYbZZZkaempgenYYYdeXXX3A8080XXX2FvocabXXX2FusZZZgaapZZZ2009ZZZ01ZZZ31XXX23Assets]})

SELECT {[httpXXX3AXXX2FXXX2FpurlYYYorgXXX2FlinkedZZZdataXXX2FsdmxXXX2F2009XXX2FmeasureXXX23obsValueAGGFUNCAVG], [httpXXX3AXXX2FXXX2FpurlYYYorgXXX2FlinkedZZZdataXXX2FsdmxXXX2F2009XXX2FmeasureXXX23obsValuehttpXXX3AXXX2FXXX2FlodYYYgesisYYYorgXXX2FlodpilotXXX2FALLBUSXXX2FZA4570v590YYYrdfXXX23dsAGGFUNCSUM], [httpXXX3AXXX2FXXX2FpurlYYYorgXXX2FlinkedZZZdataXXX2FsdmxXXX2F2009XXX2FmeasureXXX23obsValuehttpXXX3AXXX2FXXX2FestatwrapYYYontologycentralYYYcomXXX2FidXXX2Ftec00114XXX23dsAGGFUNCSUM]} ON COLUMNS, {Members([httpXXX3AXXX2FXXX2FpurlYYYorgXXX2FdcXXX2FtermsXXX2Fdate])} ON ROWS FROM [httpXXX3AXXX2FXXX2FestatwrapYYYontologycentralYYYcomXXX2FidXXX2Ftec00114XXX23dsXXX2ChttpXXX3AXXX2FXXX2FlodYYYgesisYYYorgXXX2FlodpilotXXX2FALLBUSXXX2FZA4570v590YYYrdfXXX23ds] WHERE CrossJoin({[httpXXX3AXXX2FXXX2FlodYYYgesisYYYorgXXX2FlodpilotXXX2FALLBUSXXX2FgeoYYYrdfXXX2300], [httpXXX3AXXX2FXXX2FestatwrapYYYontologycentralYYYcomXXX2FdicXXX2FgeoXXX23DE]}, {[httpXXX3AXXX2FXXX2FlodYYYgesisYYYorgXXX2FlodpilotXXX2FALLBUSXXX2FvariableYYYrdfXXX23v590_1]}}

SELECT {[httpXXX3AXXX2FXXX2FpurlYYYorgXXX2FlinkedZZZdataXXX2FsdmxXXX2F2009XXX2FmeasureXXX23obsValueAGGFUNCAVG], [httpXXX3AXXX2FXXX2FpurlYYYorgXXX2FlinkedZZZdataXXX2FsdmxXXX2F2009XXX2FmeasureXXX23obsValuehttpXXX3AXXX2FXXX2FlodYYYgesisYYYorgXXX2FlodpilotXXX2FALLBUSXXX2FZA4570v590YYYrdfXXX23dsAGGFUNCSUM], [httpXXX3AXXX2FXXX2FpurlYYYorgXXX2FlinkedZZZdataXXX2FsdmxXXX2F2009XXX2FmeasureXXX23obsValuehttpXXX3AXXX2FXXX2Folap4ldYYYgooglecodeYYYcomXXX2FgitXXX2FOLAP4LDZZZtrunkXXX2FtestsXXX2FestatwrapXXX2Ftec00114_dsYYYrdfXXX23dsAGGFUNCSUM]} ON COLUMNS, {Members([httpXXX3AXXX2FXXX2FpurlYYYorgXXX2FdcXXX2FtermsXXX2Fdate])} ON ROWS FROM [httpXXX3AXXX2FXXX2Folap4ldYYYgooglecodeYYYcomXXX2FgitXXX2FOLAP4LDZZZtrunkXXX2FtestsXXX2FestatwrapXXX2Ftec00114_dsYYYrdfXXX23dsXXX2ChttpXXX3AXXX2FXXX2FlodYYYgesisYYYorgXXX2FlodpilotXXX2FALLBUSXXX2FZA4570v590YYYrdfXXX23ds] WHERE {[httpXXX3AXXX2FXXX2FlodYYYgesisYYYorgXXX2FlodpilotXXX2FALLBUSXXX2FgeoYYYrdfXXX2300], [httpXXX3AXXX2FXXX2Folap4ldYYYgooglecodeYYYcomXXX2FdicXXX2FgeoXXX23DE]}

Drill-across(

Rollup (Slice (Dice (Projection (BaseCube (http://olap4ld.googlecode.com/git/OLAP4LD-trunk/tests/estatwrap/tec00114_ds.rdf#ds), {http://purl.org/linked-data/sdmx/2009/measure#obsValueAGGFUNCAVG, http://purl.org/linked-data/sdmx/2009/measure#obsValuehttp://olap4ld.googlecode.com/git/OLAP4LD-trunk/tests/estatwrap/tec00114_ds.rdf#dsAGGFUNCSUM}), (http://lod.gesis.org/lodpilot/ALLBUS/geo.rdf#list = http://lod.gesis.org/lodpilot/ALLBUS/geo.rdf#00) OR (http://lod.gesis.org/lodpilot/ALLBUS/geo.rdf#list = http://olap4ld.googlecode.com/dic/geo#DE)), {http://ontologycentral.com/2009/01/eurostat/ns#aggreg95, http://ontologycentral.com/2009/01/eurostat/ns#geo, http://ontologycentral.com/2009/01/eurostat/ns#indic_na}), {}), 


Rollup (Slice (Dice (Projection (BaseCube (http://lod.gesis.org/lodpilot/ALLBUS/ZA4570v590.rdf#ds), {http://purl.org/linked-data/sdmx/2009/measure#obsValueAGGFUNCAVG, http://purl.org/linked-data/sdmx/2009/measure#obsValuehttp://lod.gesis.org/lodpilot/ALLBUS/ZA4570v590.rdf#dsAGGFUNCSUM}), (http://lod.gesis.org/lodpilot/ALLBUS/geo.rdf#list = http://lod.gesis.org/lodpilot/ALLBUS/geo.rdf#00) OR (http://lod.gesis.org/lodpilot/ALLBUS/geo.rdf#list = http://olap4ld.googlecode.com/dic/geo#DE)), {http://lod.gesis.org/lodpilot/ALLBUS/vocab.rdf#geo, http://lod.gesis.org/lodpilot/ALLBUS/vocab.rdf#variable}), {}))

... Now, we see that we need to start integrate the datasets. How to do that, entity consolidation 
as in ISEM paper?

===Use dataset name in measure name (as in ISEM paper)===
* Had to replace dataset name with "" in property and variable.
* Currently only for SUM.

===Problem: Drill-Across over cubes with same measures===
* Problem is that in the MDX query, there is no way to distinguish between the measures.
* Possible solutions
** Decide from the ordering: The first obsValue, the second obsValue. However, this would need to be generic.
** Merge measures: In resulting cube, only have one measure. If there are several facts that have the same dimension member combination, we would throw an error. This would allow to union cubes. 
** Further automatically distinguish measures by the cube. Thus, not only sdmx-measure:obsValueAGGFUNCAVG and sdmx-measure:obsValueAGGFUNCCOUNT and sdmx-measure:obsValue, but sdmx-measure:obsValueCube1AGGFUNCAVG ...

Question:

* How many measures does the "global cube" have? Drill-Across should be defined as combining measures from several cubes (otherwise, would make it more complicated as is). Processing-wise, it would not make a difference. The global cube should have measures for each single measure in the cubes. This is how we defined it in ISEM paper: Note, in order to compare metrics from separate cubes - different from Dimensions and Members - Measures always need to denote different metrics, even though they may be described by the same property or linked by owl:sameAs. Thus, how about: 

Problem:

ALLBUS/ZA4570v590.rdf defines an rdf:range of xsd:decimal, Estatwrap defines an rdf:range of xsd:double. As a result, the metadata queries for measures return two different measures with the same name. 

Solution:

The real problem is that publishers use sdmx-measure:obsValue differently. They set a range, although the official publishers have not done that. In my opinion, in this case, publishers should create sub-properties of sdmx-measure and then "extend" it.

As a workaround, for now, we simply always assume xsd:decimal as data-type. However, this is a quick-fix.

Question: What dataset does the measure httpXXX3AXXX2FXXX2FpurlYYYorgXXX2FlinkedZZZdataXXX2FsdmxXXX2F2009XXX2FmeasureXXX23obsValueAGGFUNCAVG use? It results in this:

+--+------+-------------+
|  | 2005 |             |
|  | 2006 |       93.86 |
|  | 2007 |             |
|  | 2008 |       95.28 |
|  | 2009 |             |
|  | 2010 |             |
|  | 2011 |             |
|  | 2012 |             |
|  | 1980 |             |
|  | 1991 |             |
|  | 1992 |             |
|  | 1994 |             |
|  | 1996 |             |
|  | 1998 |             |
|  | 2000 |             |
|  | 2004 |             |

The new measure httpXXX3AXXX2FXXX2FpurlYYYorgXXX2FlinkedZZZdataXXX2FsdmxXXX2F2009XXX2FmeasureXXX23obsValuehttpXXX3AXXX2FXXX2FestatwrapYYYontologycentralYYYcomXXX2FidXXX2Ftec00114XXX23dsAGGFUNCSUM does not result in anything.

In summary, the problem is that: Either we do select a general measure. Then, both OLAP-2-SPARQL algorithms return results. Nested loop would find the overlaps and probably return the first datasets measure since the global cube does ...? Good question.

The problem is the following: The "global" cube describes all measures, dimensions etc. But Drill-Across assumes the metadata of the first dataset. This is wrong since we need the union of measures etc. 

Solution: The Metadata about the "global" cube needs to be a super-set of the Metadata of the resulting Drill-Across operators. Especially, since Drill-Across can be nested, also. Thus, we need to distinguish:

* createMetadata()
* createData()

One thing I do not like currently: 

1) Partly, we create the metadata using SPARQL queries in metadata queries of Linked Data Cubes Engine. 
2) Partly, we create the metadata directly on given metadata in analytical operators.

The thing is that the analytical operators create many more different data cubes, e.g., slices of the overall data cubes.

Therefore, we probably cannot find synergies, here. 

Plan: 

To have the operators create the metadata, also.

Allow Base-Cube, to also return metadata of the "global cube".

Unfortunately, metadata queries with their "restrictions" are still different from simply loading the cube metadata. Therefore, this refactoring is postponed.

Instead, I go ahead and add the metadata processing to Drill-Across iterator.

Quick-fix for now: The measures are created specifically, all others are not.

If I ask for the same measure twice, the result would always be the first measure (since we cannot distinguish
between different datasets)

Therefore, next goal is to -- similar to ISEM paper -- to also use the dataset name in the measure name.

==2014-04-30==
* Works now. Problems were:
* Not clear what infos are extracted from original MDX parse tree.
* Now, measure list is created from MDX parse tree.
* Measure list is list of members, so, no header.

==2014-04-24==
* Still problem of Drill-Across
* Status (Query + Result)

Drill_across_QueryTest

<pre>

Logical plan:Drill-across(Slice (BaseCube (http://olap4ld.googlecode.com/git/OLAP4LD-trunk/tests/estatwrap/tec00114_ds.rdf#ds), 

{http://ontologycentral.com/2009/01/eurostat/ns#aggreg95, http://ontologycentral.com/2009/01/eurostat/ns#indic_na}), 

Slice (BaseCube (http://olap4ld.googlecode.com/git/OLAP4LD-trunk/tests/estatwrap/tsdec420_ds.rdf#ds), 

{http://ontologycentral.com/2009/01/eurostat/ns#sex}

))

+ 

http://olap4ld.googlecode.com/dic/geo#US; 2011; 148.0; 1; 148; 70.433333333333333333333333;

</pre>

Example_QB_Datasets_QueryTest.testDrillAcrossEstatwrapGDPpercapitainPPS_EurostatEmploymentRate

<pre>

Logical query plan: Drill-across(

Rollup (Slice (Dice (Projection (BaseCube (http://olap4ld.googlecode.com/git/OLAP4LD-trunk/tests/estatwrap/tec00114_ds.rdf#ds), 

{http://purl.org/linked-data/sdmx/2009/measure#obsValueAGGFUNCAVG, http://purl.org/linked-data/sdmx/2009/measure#obsValueAGGFUNCCOUNT}), {}), 

{http://ontologycentral.com/2009/01/eurostat/ns#aggreg95, http://ontologycentral.com/2009/01/eurostat/ns#indic_na}), {}), 

Rollup (Slice (Dice (Projection (BaseCube (http://olap4ld.googlecode.com/git/OLAP4LD-trunk/tests/estatwrap/tsdec420_ds.rdf#ds), 

{http://ontologycentral.com/2009/01/eurostat/ns#employment_rate}), {}), 

{http://ontologycentral.com/2009/01/eurostat/ns#sex}), {}))

+


|  |   US   |  | 2005 |       159.0 |       159.0 |            159.0 |
|  |        |  | 2006 |       155.0 |       155.0 |            155.0 |
|  |        |  | 2007 |       152.0 |       152.0 |            152.0 |
|  |        |  | 2008 |       147.0 |       147.0 |            147.0 |
|  |        |  | 2009 |       147.0 |       147.0 |            147.0 |
|  |        |  | 2010 |       148.0 |       148.0 |            148.0 |
|  |        |  | 2011 |       148.0 |       148.0 |            148.0 |

</pre>
 

==2014-04-02_v2==
* Towards paper: Derived datasets 
* Refactored Visitor and Iterator names.
* Now, we have: 
** Visitor and Iterator for Drill-across and OLAP-to-SPARQL.
** Visitor and Iterator for Derived Datasets, Convert.
* See /media/84F01919F0191352/Projects/2014/paper/paper-macro-modelling/experiments/ for dumps created in between.

==2014-04-02==
* Drill-across operator and iterator work now. However, note, that for iterator neither reasoning is done nor arbitrary ordering of shared dimensions in result is considered.
* Bugfix in Olap2SparqlAlgorithmIterator: Not properly updated measures metadata.
* Working on test case: testDrillAcrossEstatwrapGDPpercapitainPPS_EurostatEmploymentRate
* Question: Do we not need to make DrillAcrossSparqlIterator properly update the metadata? Do we need to create the metadata of a new cube, just like we do for metadata queries? 

<pre>
The problem with Drill-across currently is that the initial query plan adds all projected measures and adds all sliced dimensions.

Also, the initial query plan uses elements from the global dataset, so that we cannot distinguish between
measures from different datasets.

However, need to make sure that only existing measures are projected. 

Solution: Before projection, check whether existing in data cube.

However, there we have the problem that the initial query plan uses Base-cube...

The key problem is in createInitialQueryPlanPerDataSet() where we ask the Linked Data Engine for the metadata of each single dataset, 
but also consider the metadata of "metaData.cube".

</pre>

* Question: When and how is metaData.cube set? It is set when creating the metadata. The metadata queries now also create a global cube.
* Question: Where lies the error?
** Maybe the metadata is not correctly build? How can I check?

* Question: Do I need to make the DrillAcross Iterator return properly modelled metadata?

* Problem: Our OLAP-to-SPARQL algorithm needs all SlicesRollups dimensions to be part of the queried data cube. However, our slicedDimensions is created for all datasets when creating the initial logical query plan. 

* Problem: All multidimensional elements in MDX query first belong to the global cube. Now, before being used in the query to the single data cube, we have to check that they actually refer to an element of the dataset.

* This is the case for projections, where we specifically project for all existing measures.

* This also is the case for slices, where we slice for any dimensions from the global cube not mentioned in query.

* How to deal with this issue?
** OLAP-to-SPARQL algorithm has a well-defined meaning.
** We can either create a correct initial query plan
** Or, we can later adapt the initial query plan
** Or, when we create the inputs to the OLAP-to-SPARQL algorithm, we ignore some parts of the query plan

* I think, if possible, we work on the initial query plan.
* Thus, next time, we try to filter non-existing projected measures and sliced dimensions.


==2014-03-22==
* Metadata queries can return global cube

==2014-03-18==
* For the first time, a GDP per capita calculation could be done in one go.

==2014-03-05==
* Refactored LDCEs:

What LogicalOlapOperatorQueryPlanVisitor do we have?

* Olap2SparqlSesameDerivedDatasetVisitor <= Was extended for Drill-across but now is more going towards "deriving datasets". Maybe, only for ConvertContext, we should materialise a new dataset.

* Olap2SparqlSesameVisitor <= Should also for LDCX eventually support Drill-across but for now does not properly define logical operators. Assumes that rollup is executed for all levels, even if not rolled-up.

To have several engines has the disadvantage that they share some functionality/logic, e.g., metadata queries, loading of cubes.

If I see this correctly, we may for now have one engine that uses different "LogicalOlapOperatorQueryPlanVisitors".

This one and only EmbeddedSesame engine includes possible preloads() in Iterators. Preloads() use LoadCube(). LoadCube() is provided as a method. Also provided as methods are Normalise() and IntegrityConstraints(). They should be run, after one or more datasets have been loaded.

LoadCube() can later be better implemented using Data-Fu. I assume that the current loading is better than the old.

We may also add getCubes() as Operators/Iterators.

We now include both "implicit measures" as well as measures without explicit aggregation function. We have also added these measures as members to getMembers().

==2013-11-05==
* Added example performance evaluation files in "testresources"
* fixed error spelling of integrity check IC-8 of spec and continued working on slicer 
* slicer can now query for certain slices (see first test)

==2013-10-23==
Refined:
* LDCX_Performance_Evaluation_LogParse_Experiments.java - Parsing log files of olap4ld and retrieving
events for interesting metrics.
* LDCX_Performance_Evaluation_XmlaTest.java - Issuing workload for performance evaluation
* LDCX_User_Study_Evaluation_XmlaTest.java - Test Case for User Study queries.

==2013-10-20==
* More fine granular logging that can be parsed using regex as done in LDCX_Performance_Evaluation_XmlaTest_LogParse_Experiments.
* LDCX_Performance_Evaluation_XmlaTest with tests for performance evaluation
* LDCX_Performance_Evaluation_XmlaTest_LogParse_Experiments with experiment using exrunner by
Gnter Ladwig for inserting log parser results into a sqlite dabase for querying and visualising
using gnuplot.
* Refined one of the example datasets (Eurostat).
* Added example for exrunner by Gnter Ladwig: SortExperiments.java
* Exrunner requires (not uploaded)
** exrunner-0.0.1-SNAPSHOT.jar
** guava-15.0.jar
** sqlite-jdbc-3.7.15-SNAPSHOT-2.jar

==2013-10-3==
* Pooling problem workaround implemented: http://linked-data-cubes.org/index.php/Olap4j-xmlaserver#Problem:_Pooling_of_data_sources

==2013-10-1==
* Pooling problem investigated, see: http://linked-data-cubes.org/index.php/Olap4j-xmlaserver#Problem:_Pooling_of_data_sources

==2013-09-29==
* We now throw more OlapExceptions in Linked Data Engines (Sesame), since we hope that this
makes the system more robust in case of errors.
* A prepared olap statement is used by XmlaHandler.java 
(statement = connection.prepareOlapStatement(mdx);). Afterwards the cellset is 
created (cellSet = statement.executeQuery();). This means in our implementation, that the cellset
is populated twice and the linked data engine is queried twice (and logging is done, twice). Thus,
we have adapted cellset to only populate the cells when actually cells are asked for; otherwise, only
the metadata is created and can be queried from the prepared olap statement if necessary.
* Unfortunately, the prepared statement does not help us in any way to save resources. Therefore, we
have disabled it in Olap4ldPreparedStatement.java. 

==2013-09-28==
* More detailed logging, per connection. Unfortunately, no session tracking, yet.
* Better labels for SSB dataset

==2013-09-27==
* TODO: 
** Better understanding for users. Unfortunately, currently there is a JavaScript error in LDCX.
** Even more robustness? 
*** Currently, if I cannot store any more in the triple store, I just throw an error. After reload, a new triple store should be filled?
*** If a query is too big, java script LDCX currently does not complain, can be improved?
*** According to Gnter und Daniel, asking for the amount of free memory is not a best practice. 
Instead 1) I should properly test the system and see whether there are errors 2) I should put user
restrictions on the system, e.g., you cannot analyze datasets with such and such amount of triples.
**** Added triple count check and magic number 100,000 triples max.

** LDCX pivot visualisation often wrong (first column, if we use several dimensions on rows?)

==2013-09-26==
* Why does Edgar example dataset does not fulfill constraint 12? [http://www.w3.org/2011/gld/validator/qb/qb-test?upload=upload-2013-09-25T20-51-51-104] 
** This is in more detail described here: http://linked-data-cubes.org/index.php/Displaying_of_background_information_useful_for_interpretation
** Edgar would need to be improved

* Robustness
** The system is still not very robust, since queries may ask for too many columns and rows to be visualized (in JavaScript on client? Or to populate result set on server?). 
*** One solution might be JSON as described here: http://www.blogger.com/comment.g?blogID=5672165237896126100&postID=9002652898272515547
*** Another solution would be to properly implement NON EMPTY COLUMNS and NON EMPTY ROWS and to set this up as default in the system. This way, no impossible queries for a non too big file should be possible.
*** Question: Does olap4ld cope with a big query?

* For robustness: Properly implement NON EMPTY CLAUSE:
** Make test case: We extend ssb example. <= result: NON EMPTY works just fine, the problem is rather the huge XMLA result for large queries. Any cell that is visible is mentioned in the xmla result.
** More info see http://linked-data-cubes.org/index.php/NON_EMPTY_Clause_in_MDX
** As a result, we should be using NON EMPTY for both rows and columns.

==25 Sep 2013==
* DONE
** For XMLA Cube Discovery (and later, Dimension-Discovery...), I want to load in the respective 
metadata (and, for now, data). For a query, however, I do not necessarily want to load everything
again.
*** Caching is done in two cases: 1) By filling the triple store. 2) By populating 
the metadata object lists.
**** 1) Currently, a triple store is created when the EmbeddedSesameEngine is created which is
done when the Driver is loaded which should be only once per XMLA-Connection. The triple store 
is set back (rollback), when Connection.setCatalog() or Connection.setSchema() are called which is done
in xmlaserver every time in RowsetDefinition a XmlaResponse is populated (populateImpl()). 
For instance: populateImpl -> setCatalog() -> getCatalog() -> populateCatalog() -> 
catalog.getSchemas() -> schema.getCubes() -> populateCube() -> cube.getDimensions() -> 
populateDimension() -> dimension.getDescription() -> dimension.getHierarchies() -> 
firstHierarchy.getLevels()... <= TODO: If we assume that Discover.Cubes is called at the very beginning, 
the triple store should not be rollback for every discover method. It actually should never be
generally rolled back but dynamically "updated" (there is no open rollback functionality): 
1) Disable rollback. 2) Populate repository as much as possible and if we cannot populate anymore, 
give error. For now, lets see when a new repository is created.  
**** 2) The metadata object lists currently are deferred lists which get populated 1) if they are 
new. 2) if explicitly a cube is asked for.
** Test Cases do not work any more with constraints (should I disable them, again?)
*** Instead of adding new normalization algorithms (first done, but commented for skos), I changed
the example of ssb001.
** Problem: Some integrity constraints are too complex to be run on larger datasets, esp. those
which need to touch every observation (e.g., IC-12). 
*** Possible solutions: 1) For now, we cannot run these integrity constraints and comment them. 2) We only run
them if the size of the dataset is small and "small" is depending on the size of memory. 
For instance, if free memory size is twice as big as current load of data. 3) We
only run them in a specific cube dataset debugging mode.
*** Current solution: Had to use 3) since 2) did not work.
*** Planned solution: Maybe own better implementation of integrity checks or use more memory 

==22 Sep 2013==
* Why does Eurostat example agg dimension is not shown? There, we should have a test case. 
Solution: Had to define range and datatype for measure. Done for example.
* For eurostat example, I also had to adapt the dimension names (which were ontologycentral).
** Also, had to adapt so that codelist and observation coincidate. Why did we not realize that observation values did not align with code list values?
* Since the error before has not been detected, I added more integrity constraints from spec
** watch out, also inScheme had to be done to fullfil integrity constraint

==21 Sep 2013==
* Problem: Apparently, loading "http://estatwrap.ontologycentral.com/id/../data/tec00114" fails, 
whereas loading "http://estatwrap.ontologycentral.com/data/tec00114" does not.
* Improved logging, see http://www.linked-data-cubes.org/index.php/Logging_Olap4ld
* new estatwrap example: http://olap4ld.googlecode.com/git/OLAP4LD-trunk/tests/estatwrap/tec00114_ds.rdf#ds
** had to adapt it: every observation links to #ds. Structure has certain uri (with hash #).
* added SMARTDBWRAP example
* refined test suite Example_QB_Datasets_QueryTest

==20 Sep 2013==
* LDCX should be working on example datasets
** Estatwrap is mostly offline. Therefore, we created example datasets.
** Edgar, unfortunately, most filings cannot be represented, currently.
** Show proper dataset name for Edgar

* Improve LDCX
** Monitoring usage (Session + Logical Query + Successful/Unsuccessful error) = normal running logger.
** Logging levels: 
    FINEST <= not used
    FINER <= not used
    FINE <= not used
    CONFIG <= debugging
    INFO <= usage monitoring (only the most important bits and pieces)
    WARNING <= possible errors
    SEVERE <= not used


* More robust w.r.t. memory usage
** Heap space bigger?
** Build in security that too big files are not loaded and that we store not too many triples in the repository (maybe should be possible to setup) 

* Demo should work on hierarchies, also
** Extend ssb example with skos:ConceptScheme possibilities

==19 Sep 2013==
* Have been working on Edgar dataset example
* Problem: Suddenly, LDCX did not work, anymore. Solution: There was an error in the JavaScript. Use Firebug and look at error console.
* Repository is now rollback() if catalog or schema are set, not get.
* Added new version of Example Edgar dataset
** Allow any Edgar dataset from feed - *not necessary for demo*

==16 Sep 2013==
* Problem: If we create measures based on a measure without aggregation function, and we select several measures,
we do not create separate graph patterns. *solved*
* olap4ld tests were running endless since parentuniquemember with [null] had to be searched for.
Bug of having [null] names for "null"/null (according to spec) removed in Connection.Handler and Olap4ldUtil. 
* Status: Yahoo! Finance tests are not successful. Yes, they are, there apparently was some temporal error.

==12 Sep 2013==
* When transforming an MDX query to a query in MdxMethodVisitor, we distinguish whether a cube
is identified or not, via this.cube that is stored when visiting the from clause.

==8 Sep 2013==
* Problem: 
** How to ask for representation of Physical Query Tree?
*** simply allowing to ask LDE for last Physical Operator Tree
*** OR: How about defining better logging mechanism? 
*** Currently, we define logging level at Driver. We now try to log to a file.
* More logging in Slicer

==6 Sep 2013==
* Slicer_QueryTest.java [1] now executes all 1-dimensional OLAP queries to example cube.

==5 Sep 2013==
* Restrictions now use URI representation, but also are able to convert from MDX representation.
* Slicer_QueryTest.java [1] now contains test "test_example_ssb001_slicer_oneDim()", that creates 
all 1-dimensional OLAP queries to example cube

==2 Sep 2013==
* Renamed LD_Cubes test case to "Example_QB_Datasets_QueryTest".
* Logical Olap Op DiceOp now receives List<Node[]> of cube that is
used in query processing.
* removed all errors in sources
* Deactivated not successful tests in Example_QB_Datasets_QueryTest.
* Logical Olap Op SliceOp and RollupOp now receive List<Node[]> of cube that is
used in query processing. 
* OlapOps are now completely using Node[] with URI representation (not MDX representation) as inputs

==31 Aug 2013==
* Logical Olap Op ProjectionOp now receives List<Node[]> of cube that is
used in query processing.

==30 Aug 2013==
* Logical Olap Op BaseCube now receives List<Node[]> of cube that is
used in query processing.
* Status of junit test LD_Cubes_Explorer_QueryTest.java: Only tests
related to the setDatabase problem and Edgar loading do not succeed.

==2013-08-16==
* Added methods to metadata objects to get List<Node[]> representation to be used in olap 
operators (as an interface to Linked Data Engine).

==2013-08-15==
* Logical OLAP Operators (http://www.linked-data-cubes.org/index.php/Olap4ld_Query_Optimizer)
* Changed Visitor to LogicalOlapOperatorPlanVisitor

==2013-08-14==
* If now aggregation function is found, we automatically create new measures with certain
aggregation functions (simply added aggregation fnct name to the end). 
However, WATCH OUT, BIND-syntax of SPARQL is complicated: You should not be
using the variable that you are binding to in other patterns (see, e.g., ?MEASURE_AGGREGATOR_PART)

==2013-08-07==
* Improved exception handling. Now, LinkedDataEngine also returns OlapExceptions which are forwarded
to the using engine in order to make transparent errors to users.

==2013-07-12==
* Removed sesame libraries. Sesame is to be added externally when using sesame engines in olap4ld

==2013-07-11==
* Full spec tests on test data
* Full spec test better loging for users, independent from logging level. Better log-out.

==2013-07-10==
* Eurostat examples. Have English labels.
* Extended test cases
* Retrieving of data so far very simple: 
** rollback every time we ask the database catalog for its schema.
** loading of uris if we check whether something is a cube; loading ds, then dsd.
** loading of other uris just by coincidence if we check if they are cubes.

==2013-07-07==
* Nested crossjoins are possible, now.
* Examples listed in demo
* ld-cubes-explorer added
* Problem: Have caption shown instead of elements.

==2013-07-05==
* Problem: GetCubes() auf dem Schema gibt uns keinerlei Cubes, da das System nicht wei, woher
er sie nehmen soll. Daher setzen wir auf getMeasures(Cube), getDimensions(Cube) etc., mssen hier
jedoch sicherstellen, dass zumindest der Datensatz aufgelst worden ist.
** Lsung: We check the restrictions and at least check whether dataset is resolved (in which case,
the datastructuredefinition should have been resolved as well).
* with setCatalog() in Connection, I reset the LinkedDataEngine from now on. <= does not work!
** We reset at getSchemas() of Catalog. Also, we now load only if instance qb:DataSet.
* We inserted checks from specification for when qb:DataSet is loaded.
* Problem: Beim Identifizieren von Identifiern, muss ich durch alle elemente gehen, dabei frage ich 
auch, ob es sich um einen Cube/DS handelt. Lsung: Bevor ich es mache, berprfe ich erst, ob es sich 
tatschlich um ein DS handelt.

==2013-07-4==
* Problem came up that the following elements have the same name in our current model:
** Dimension, Dimension without codelist Hierarchy, Dimension without codelist Level
** Hierarchy, Regular hierarchy level
** Since we do not need to refer to those sub-elements, directly, but rather will be asking for all 
members of the dimension, we do not need to consider this.
* Measure DataType is now interpreted from rdfs:range.
* Watch out for the right name in MDX query.
* Sesame SPARQL adaption:
** Picky with empty spaces, added serveral
** (COUNT(?variable) as ?variable)
* xmla-server + xmla4js was quite picky about some non-implemented methods in olap4ld taken over 
from olap4j/mondrian (e.g., Olap4ldDatabase.getUserName()). Fixed those with workarounds. 
* We name Catalog and Schema the same due to compatibility reasons with xmla-server/xmla4js
* We have added measureList to CellSet as added metadata.

==2013-07-03==
* xml2nx adapted to Sesame SPARQL/XML output with linebreaks
* added manually created representative SSB example
* added Sesame SPARQL templates

==2013-07-02==
* Read in first location.
* Problem: A data cube is an instance of qb:DataSet. What do we
typically store with the qb:DataSet? For sure the outgoing link to dsd,
the outgoing link to all of its slices that in turn link to the
observations. Sometimes, we may have the observations already in the ds
location.
* Ideally, we get the following from...
** DS: Links from observations / Links to slices that in turn link to observations, rdfs:label, 
rdfs:comment, qb:structure.
** DSD: All information about the DSD, including code lists.

* For now, we assume (and adapt the SSB dataset accordingly):
** DS: Links from observations, rdfs:label, qb:structure
** DSD: All measure properties, dimension properties, code lists...

* Note in spec: If a dimension property has a qb:codeList, then the value of the dimension property on every qb:Observation must be in the code list. 
* Note in spec: Every qb:Observation has exactly one associated qb:DataSet. 
* Note in spec: Every qb:DataSet has exactly one associated qb:DataStructureDefinition. 
* Note in spec: "Every dimension declared in a qb:DataStructureDefinition must have a declared rdfs:range."
* Note in spec: "Every dimension with range skos:Concept must have a qb:codeList." <= This means, 
we do not necessarily need a code list in many cases. But, if we have a code list, then:
* "If a dimension property has a qb:codeList, then the value of the dimension property on every 
qb:Observation must be in the code list."

==Status 2013-07-04==
* Decided for own example based on example in spec:
** sdmx dimension: http://publishing-statistical-data.googlecode.com/svn/trunk/specs/src/main/vocab/sdmx-dimension.ttl
** code list sex: http://publishing-statistical-data.googlecode.com/svn/trunk/specs/src/main/vocab/sdmx-code.ttl#sex
** folder of own online linked data files: http://olap4ld.googlecode.com/git/OLAP4LD-trunk/tests/ssb001/ttl/

==Status 2013-07-03==
* Note in spec: Every qb:DataSet has exactly one associated qb:DataStructureDefinition. 
* Note in spec: "Every dimension declared in a qb:DataStructureDefinition must have a declared rdfs:range."
* Note in spec: "Every dimension with range skos:Concept must have a qb:codeList." <= This means, 
we do not necessarily need a code list in many cases. But, if we have a code list, then:
* "If a dimension property has a qb:codeList, then the value of the dimension property on every 
qb:Observation must be in the code list."
* Note in spec: Every qb:Observation has exactly one associated qb:DataSet.

==Status 2013-06-26==
* Added SSB_001 test data that I simply shortended. Can be looked up via: http://olap4ld.googlecode.com/git/OLAP4LD-trunk/tests/ssb001/ttl/custlevels.ttl#dsd
* Added [http://olap4ld.googlecode.com/git/OLAP4LD-trunk/tests/ssb001/ttl/custlevels.ttl#dsd] as prefixes (We need a better solution, here)
* Worked now further on http://intra.b-kaempgen.de/dawiki/index.php/Creating_an_OLAP4LD_test_environment_and_using_Sesame 
or now at http://www.linked-data-cubes.org/index.php/EmbeddedSesameEngine 
* Changed the fact that if specific element of DefferredList is queried, we do not "populate".


==2013-06-26 (before)==

* Warum haben die Dimensionen alle zwei Englische rdfs:label? 

===Problem: Saiku calls <Measures>===
PREFIX dc: <http://purl.org/dc/elements/1.1/> 
PREFIX sdmx-measure: <http://purl.org/linked-data/sdmx/2009/measure#> 
PREFIX edgar: <http://edgarwrap.ontologycentral.com/vocab/> 
PREFIX gesis-dbpedia-stats2: <http://lod.gesis.org/dbpedia-stats/> 
PREFIX smartdbwrap: <http://smartdbwrap.appspot.com/> 
PREFIX qb: <http://purl.org/linked-data/cube#> 
PREFIX refgovukyear: <http://reference.data.gov.uk/id/year/> 
PREFIX refgovukmonth: <http://reference.data.gov.uk/id/month/> 
PREFIX dcterms: <http://purl.org/dc/terms/> 
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> 
PREFIX skosclass: <http://ddialliance.org/ontologies/skosclass#> 
PREFIX dropedia: <http://agkwebserver2.agk.uni-karlsruhe.de/~dropedia/index.php/Special:URIResolver/> 
PREFIX eus: <http://ontologycentral.com/2009/01/eurostat/ns#> 
PREFIX smartlocation: <http://smartdbwrap.appspot.com/id/location/> 
PREFIX dbpedia: <http://dbpedia.org/resource/> 
PREFIX owl: <http://www.w3.org/2002/07/owl#> 
PREFIX smartanalysisobject: <http://smartdbwrap.appspot.com/id/analysisobject/> 
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> 
PREFIX rdfh: <http://lod2.eu/schemas/rdfh#> 
PREFIX dropedialocal: <http://localhost/Dropedia/index.php/Special:URIResolver/> 
PREFIX skos: <http://www.w3.org/2004/02/skos/core#> 
PREFIX gesis-dbpedia-stats: <http://lod.gesis.org/dbpedia-stats/ns#> 
PREFIX refgovukday: <http://reference.data.gov.uk/id/day/> 
PREFIX rdfh-inst: <http://lod2.eu/schemas/rdfh-inst#> 
select "LdCatalog" as ?CATALOG_NAME "LdSchema" as ?SCHEMA_NAME ?CUBE_NAME ?DIMENSION_UNIQUE_NAME ?DIMENSION_UNIQUE_NAME as ?HIERARCHY_UNIQUE_NAME ?DIMENSION_UNIQUE_NAME as ?LEVEL_UNIQUE_NAME ?DIMENSION_UNIQUE_NAME as ?LEVEL_CAPTION ?DIMENSION_UNIQUE_NAME as ?LEVEL_NAME "N/A" as ?DESCRIPTION "1" as ?LEVEL_NUMBER "0" as ?LEVEL_CARDINALITY "0x0000" as ?LEVEL_TYPE  from <http://fios:saiku>  where { ?CUBE_NAME qb:component ?compSpec . ?compSpec qb:dimension ?DIMENSION_UNIQUE_NAME. FILTER NOT EXISTS { ?DIMENSION_UNIQUE_NAME qb:codeList ?HIERARCHY_UNIQUE_NAME. }   FILTER (?CUBE_NAME = <http://public.b-kaempgen.de:8080/archive/MA/2007-06-06#dsd>)  FILTER (?DIMENSION_UNIQUE_NAME = <Measures>) } order by ?CUBE_NAME ?DIMENSION_UNIQUE_NAME ?LEVEL_NUMBER 


===Open issues===

* Reasoning does not work
** If we enable reasoning, the queries take much too long
** Also, we do not know what URIs are given to the members.
** Possible solution: We need to further normalise the data to use one specific uri per resource.

* Queries are very slow. Possible solutions:
** We could define code lists (hierarchies etc.)
** We could enable caching
** We could only gradually populate the multidimensional data

** How about predefining codelists?

* CAPTIONS
** We only use english captions
** We use skos:notation for members
** We use rdfs:label for everything else

* How about querying over several cubes together?
** We have to define the measures clearly. For SEC / Yahoo what is the appropriate aggregation function?
** Solution: AVG seems a good candidate for now. 
** Another solution: We separate the measures.

* How about the Member function?

* How about the time information?
** One part of the problem: In the current implementation, we cater for the special situation where
the observations use Literal values but we still would like to define code lists. In those cases,
we so far allowed to use skos:Concept and to give them skos:notations that could be used in the
observations. In FIOS 2.0, this would be the case for dates and for segment. However, we already use
skos:notation as a (unique) caption for a member. Therefore, we change this assumption and take
Literal observations automatically as "degenerated dimensions".
** Also, I am not sure whether we want to model dates as Literal dates. How about modelling "temporal 
coverage" with an own uri, maybe giving it ical:dtstart, ical:dtend as descriptive properties, but 
most importantly giving it skos:notation with "Period from dtstart to dtend" or "Instant at dtstart"?
Also, one could then more or less automatically link to the date reference ontology for hierarchies,
e.g., within year 2011 or before 2012. Also, we could give it names such as "Q3, Q4" etc.
** ...

===How about querying over several cubes together?===
* Cube created
* Test query created
* Next steps
** Have to query over both datasets together
** Enable reasoning
** Try in Saiku


==2013-02-22==
* We have again refactored the use of square brackets: For "Measures", we keep the name without 
square brackets since Saiku is using those names specifically. For all others, the square brackets
are a necessary part of the name (e.g., since MDX parser would not work for a member that starts 
with a numeric character.

==2013-02-17 - Problem: .Member is interpreted as an Identifier segment==
* uri.Member is interpreted as a full Identifier of several segments
* Solutions: 
** How about defining a visitor that goes through the parser tree and transforms 
IdentifierNode(uri.Members) to CallNode(Members, Identifier(uri)).

==2013-02-18 - Problem: Literal values apparently require square brackets==
* MDX Parser does not work with 2009ZZZ08ZZZ31, only with [2009ZZZ08ZZZ31].
* Also, Saiku does not attach [] to the names when transformed to MDX.
* Therefore, the unique name should actually say [Measures].
* Again, we need to rethink the brackets approach:
** Since the unique name is used for building MDX queries, and since MDX requires it, we add it.
** When coming from MDX, we need to use toString(), since it keeps the square brackets.
** Also, we need to add [] if comparing.

==2013-02-17 - Problem: If asking for members, we do not consider Literal values==
* Of all multidimensional elements, members possibly can be described by Literal values.
* Thus, when querying for those members, we have to consider that.
* In theory this can happen in two cases:
** A codeList is described that defines concepts with skos:notation (not overly discussed, yet)
** No codeList is given, and the members are simply read from the observations (in the future, 
this case might be abandoned, since for cubes, we should always have a complete code list. However, 
in data cubes, often you have "degenerated" dimensions, thus we might consider keeping it)
* Problem: How do we know whether it is a URI member or Literal member? For now, we try just
guessing from the name. However, with prefixeduris this will not work.

* TODO: SPARQL query still does not work:
** Problem has been the fact that str(?MEMBER_UNIQUE_NAME) was needed.

==2013-02-16 - Still problem with squared brackets==
* Solution: Now that we fully encode a name (even dots), we do not need
squared brackets any more.
* We now try whether we can fully avoid them for the names (since with
LD, their unique identifier should be unique.
* Therefore I also do not need removeSquaredBrackets any more.

==2013-02-16 - Problem: Too many cubes==
* Get Cubes, maybe reasoning?

<pre>
PREFIX dc: <http://purl.org/dc/elements/1.1/> 
PREFIX sdmx-measure: <http://purl.org/linked-data/sdmx/2009/measure#> 
PREFIX edgar: <http://edgarwrap.ontologycentral.com/vocab/> 
PREFIX gesis-dbpedia-stats2: <http://lod.gesis.org/dbpedia-stats/> 
PREFIX smartdbwrap: <http://smartdbwrap.appspot.com/> 
PREFIX qb: <http://purl.org/linked-data/cube#> 
PREFIX refgovukyear: <http://reference.data.gov.uk/id/year/> 
PREFIX refgovukmonth: <http://reference.data.gov.uk/id/month/> 
PREFIX dcterms: <http://purl.org/dc/terms/> 
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> 
PREFIX skosclass: <http://ddialliance.org/ontologies/skosclass#> 
PREFIX dropedia: <http://agkwebserver2.agk.uni-karlsruhe.de/~dropedia/index.php/Special:URIResolver/> 
PREFIX eus: <http://ontologycentral.com/2009/01/eurostat/ns#> 
PREFIX smartlocation: <http://smartdbwrap.appspot.com/id/location/> 
PREFIX dbpedia: <http://dbpedia.org/resource/> 
PREFIX owl: <http://www.w3.org/2002/07/owl#> 
PREFIX smartanalysisobject: <http://smartdbwrap.appspot.com/id/analysisobject/> 
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> 
PREFIX rdfh: <http://lod2.eu/schemas/rdfh#> 
PREFIX dropedialocal: <http://localhost/Dropedia/index.php/Special:URIResolver/> 
PREFIX skos: <http://www.w3.org/2004/02/skos/core#> 
PREFIX gesis-dbpedia-stats: <http://lod.gesis.org/dbpedia-stats/ns#> 
PREFIX refgovukday: <http://reference.data.gov.uk/id/day/> 
PREFIX rdfh-inst: <http://lod2.eu/schemas/rdfh-inst#> 
select "LdCatalog" as ?CATALOG_NAME "LdSchema" as ?SCHEMA_NAME ?CUBE_NAME "CUBE" as ?CUBE_TYPE ?CUBE_NAME as ?DESCRIPTION ?CUBE_NAME as ?CUBE_CAPTION  from <http://fios:2> where { ?ds qb:structure ?CUBE_NAME. ?CUBE_NAME a qb:DataStructureDefinition. OPTIONAL {?CUBE_NAME rdfs:label ?CUBE_CAPTION FILTER ( lang(?CUBE_CAPTION) = "" )} OPTIONAL {?CUBE_NAME rdfs:comment ?DESCRIPTION FILTER ( lang(?DESCRIPTION) = "" )} }order by ?CUBE_NAME limit 10
</pre>

==2013-02-14 - MDX-to-SPARQL debugging==
java.lang.RuntimeException: lookup on http://public.b-kaempgen.de:8890/sparql?query=PREFIX+dc%3A+%3Chttp%3A%2F%2Fpurl.org%2Fdc%2Felements%2F1.1%2F%3E+%0APREFIX+sdmx-measure%3A+%3Chttp%3A%2F%2Fpurl.org%2Flinked-data%2Fsdmx%2F2009%2Fmeasure%23%3E+%0APREFIX+edgar%3A+%3Chttp%3A%2F%2Fedgarwrap.ontologycentral.com%2Fvocab%2F%3E+%0APREFIX+gesis-dbpedia-stats2%3A+%3Chttp%3A%2F%2Flod.gesis.org%2Fdbpedia-stats%2F%3E+%0APREFIX+smartdbwrap%3A+%3Chttp%3A%2F%2Fsmartdbwrap.appspot.com%2F%3E+%0APREFIX+qb%3A+%3Chttp%3A%2F%2Fpurl.org%2Flinked-data%2Fcube%23%3E+%0APREFIX+refgovukyear%3A+%3Chttp%3A%2F%2Freference.data.gov.uk%2Fid%2Fyear%2F%3E+%0APREFIX+refgovukmonth%3A+%3Chttp%3A%2F%2Freference.data.gov.uk%2Fid%2Fmonth%2F%3E+%0APREFIX+dcterms%3A+%3Chttp%3A%2F%2Fpurl.org%2Fdc%2Fterms%2F%3E+%0APREFIX+rdfs%3A+%3Chttp%3A%2F%2Fwww.w3.org%2F2000%2F01%2Frdf-schema%23%3E+%0APREFIX+skosclass%3A+%3Chttp%3A%2F%2Fddialliance.org%2Fontologies%2Fskosclass%23%3E+%0APREFIX+dropedia%3A+%3Chttp%3A%2F%2Fagkwebserver2.agk.uni-karlsruhe.de%2F%7Edropedia%2Findex.php%2FSpecial%3AURIResolver%2F%3E+%0APREFIX+eus%3A+%3Chttp%3A%2F%2Fontologycentral.com%2F2009%2F01%2Feurostat%2Fns%23%3E+%0APREFIX+smartlocation%3A+%3Chttp%3A%2F%2Fsmartdbwrap.appspot.com%2Fid%2Flocation%2F%3E+%0APREFIX+dbpedia%3A+%3Chttp%3A%2F%2Fdbpedia.org%2Fresource%2F%3E+%0APREFIX+owl%3A+%3Chttp%3A%2F%2Fwww.w3.org%2F2002%2F07%2Fowl%23%3E+%0APREFIX+smartanalysisobject%3A+%3Chttp%3A%2F%2Fsmartdbwrap.appspot.com%2Fid%2Fanalysisobject%2F%3E+%0APREFIX+rdf%3A+%3Chttp%3A%2F%2Fwww.w3.org%2F1999%2F02%2F22-rdf-syntax-ns%23%3E+%0APREFIX+rdfh%3A+%3Chttp%3A%2F%2Flod2.eu%2Fschemas%2Frdfh%23%3E+%0APREFIX+dropedialocal%3A+%3Chttp%3A%2F%2Flocalhost%2FDropedia%2Findex.php%2FSpecial%3AURIResolver%2F%3E+%0APREFIX+skos%3A+%3Chttp%3A%2F%2Fwww.w3.org%2F2004%2F02%2Fskos%2Fcore%23%3E+%0APREFIX+gesis-dbpedia-stats%3A+%3Chttp%3A%2F%2Flod.gesis.org%2Fdbpedia-stats%2Fns%23%3E+%0APREFIX+refgovukday%3A+%3Chttp%3A%2F%2Freference.data.gov.uk%2Fid%2Fday%2F%3E+%0APREFIX+rdfh-inst%3A+%3Chttp%3A%2F%2Flod2.eu%2Fschemas%2Frdfh-inst%23%3E+%0Aselect++from+%3Chttp%3A%2F%2Flocalhost%3A8890%2FDAV%2Fssb_01_qb_ds%3E++from+%3Chttp%3A%2F%2Flocalhost%3A8890%2FDAV%2Fssb_01_qb_dsd%3E+from+%3Chttp%3A%2F%2Flocalhost%3A8890%2FDAV%2Fssb_01_qb_levels%3E+where+%7B++%3Fobs+qb%3AdataSet+%3Fds.+%3Fds+qb%3Astructure+%3Chttp%3A%2F%2Flod2.eu%2Fschemas%2Frdfh-inst%23dsd%3E.%3Fobs+%3Chttp%3A%2F%2Flod2.eu%2Fschemas%2Frdfh%23lo_custkey%3E+%3Frdfhlocustkey+FILTER%28%3Frdfhlocustkey+%3D+%3Chttp%3A%2F%2Flod2.eu%2Fschemas%2Frdfh-inst%23customer_178%3E+%29.%3Fobs+%3CMeasures%3E+%3FMeasures.+%3Fobs+%3Chttp%3A%2F%2Flod2.eu%2Fschemas%2Frdfh%23lo_custkey%3E+%3Frdfhlocustkey.+OPTIONAL+%7B+%3Fobs+%3Chttp%3A%2F%2Flod2.eu%2Fschemas%2Frdfh%23lo_revenue%3E+%3Frdfhlorevenue.+%7D%7D+group+by++%3FMeasures++%3Frdfhlocustkey+order+by++%3FMeasures++%3Frdfhlocustkey+ resulted HTTP in status code 400
	at org.olap4j.driver.ld.OpenVirtuosoEngine.sparqlOpenVirtuoso(OpenVirtuosoEngine.java:487)
	at org.olap4j.driver.ld.OpenVirtuosoEngine.sparql(OpenVirtuosoEngine.java:359)
	at org.olap4j.driver.ld.OpenVirtuosoEngine.getOlapResult(OpenVirtuosoEngine.java:2232)
	at org.olap4j.driver.ld.LdOlap4jCellSet.cacheDataFromOlapQuery(LdOlap4jCellSet.java:342)
	at org.olap4j.driver.ld.LdOlap4jCellSet.populateFromMdx(LdOlap4jCellSet.java:139)
	at org.olap4j.driver.ld.LdOlap4jStatement.executeOlapQuery(LdOlap4jStatement.java:446)
	at org.olap4j.SSBQueryTest.executeStatement(SSBQueryTest.java:184)
	at org.olap4j.SSBQueryTest.testGenericQuery(SSBQueryTest.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:130)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)

------------------------------------
Query:
Virtuoso 37000 Error SP030: SPARQL compiler, line 25: syntax error at 'from' before '<http://localhost:8890/DAV/ssb_01_qb_ds>'

SPARQL query:
define sql:big-data-const 0 PREFIX dc: <http://purl.org/dc/elements/1.1/> 
PREFIX sdmx-measure: <http://purl.org/linked-data/sdmx/2009/measure#> 
PREFIX edgar: <http://edgarwrap.ontologycentral.com/vocab/> 
PREFIX gesis-dbpedia-stats2: <http://lod.gesis.org/dbpedia-stats/> 
PREFIX smartdbwrap: <http://smartdbwrap.appspot.com/> 
PREFIX qb: <http://purl.org/linked-data/cube#> 
PREFIX refgovukyear: <http://reference.data.gov.uk/id/year/> 
PREFIX refgovukmonth: <http://reference.data.gov.uk/id/month/> 
PREFIX dcterms: <http://purl.org/dc/terms/> 
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> 
PREFIX skosclass: <http://ddialliance.org/ontologies/skosclass#> 
PREFIX dropedia: <http://agkwebserver2.agk.uni-karlsruhe.de/~dropedia/index.php/Special:URIResolver/> 
PREFIX eus: <http://ontologycentral.com/2009/01/eurostat/ns#> 
PREFIX smartlocation: <http://smartdbwrap.appspot.com/id/location/> 
PREFIX dbpedia: <http://dbpedia.org/resource/> 
PREFIX owl: <http://www.w3.org/2002/07/owl#> 
PREFIX smartanalysisobject: <http://smartdbwrap.appspot.com/id/analysisobject/> 
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> 
PREFIX rdfh: <http://lod2.eu/schemas/rdfh#> 
PREFIX dropedialocal: <http://localhost/Dropedia/index.php/Special:URIResolver/> 
PREFIX skos: <http://www.w3.org/2004/02/skos/core#> 
PREFIX gesis-dbpedia-stats: <http://lod.gesis.org/dbpedia-stats/ns#> 
PREFIX refgovukday: <http://reference.data.gov.uk/id/day/> 
PREFIX rdfh-inst: <http://lod2.eu/schemas/rdfh-inst#> 
select  from <http://localhost:8890/DAV/ssb_01_qb_ds>  from <http://localhost:8890/DAV/ssb_01_qb_dsd> from <http://localhost:8890/DAV/ssb_01_qb_levels> where {  ?obs qb:dataSet ?ds. ?ds qb:structure <http://lod2.eu/schemas/rdfh-inst#dsd>.?obs <http://lod2.eu/schemas/rdfh#lo_custkey> ?rdfhlocustkey FILTER(?rdfhlocustkey = <http://lod2.eu/schemas/rdfh-inst#customer_178> ).?obs <Measures> ?Measures. ?obs <http://lod2.eu/schemas/rdfh#lo_custkey> ?rdfhlocustkey. OPTIONAL { ?obs <http://lod2.eu/schemas/rdfh#lo_revenue> ?rdfhlorevenue. }} group by  ?Measures  ?rdfhlocustkey order by  ?Measures  ?rdfhlocustkey 

==2013-02-13 - From MDX to populated cellset==
1) MDX: 
SELECT 
<COLUMNAXIS> ON COLUMNS,
<ROWAXIS> ON ROWS,
FROM <CUBE>
WHERE <FILTERAXIS>

2) MdxMethodVisitor
<CUBE> => cube
<COLUMNAXIS> => list of positions for columns
<ROWAXIS> => list of positions for rows
<FILTERAXIS> => list of positions for filters
list of positions for columns => levels for column axis
list of positions for rows => levels for row axis
list of positions for filter => levels for filter axis

3) CellSetMetaData
cube => metadata
levels for column axis => hierarchies for column axis
levels for row axis => hierarchies for row axis
levels for filter axis => hierarchies for filter axis

4) OLAP query
cube => cube
levels for column axis + levels for row axis => slicesrollups
list of positions for filter => dices
check list of positions for mentioned members => projections

Pseudocode:

Algorithm 2: OLAP Query Generation
Input: metadata, axisList, filterCellSetAxis
Output: LdOlapQuery (cube, slicesrollups, dices, projections)

begin
  // Cube is easy
  cube = metadata.cube
  // Slicesrollups is easy
  slicesrollups = new List<Level>()
  for cellsetaxis \in axisList {
    position = cellsetaxis.positions(0)
    members = position.getMembers()
    for member \in members {
      // Only if no measure, since we collect measures later
      if (member.Type != Measure) {
        slicesrollups.add(member.getLevel())
      }
    }
  }
  // Problem Dices: From MDX, filterCellSetAxis returns a list of positions that should be ORed. Thus,
  // dices actually could be filled with positions.
  dices = filterCellSetAxis.positions
  // Problem Projections: We use a set, since we only want to have each Measure kept in there once.
  projections = new Set<Member>()
  for cellsetaxis \in axisList {
    for position \in cellsetaxis.positions {
      members = position.getMembers()
      for member \in members {
        if (member.Type == Measure) {
          projections.add(member)
        }
      }
    }
  }
  for position \in filterCellSetAxis.positions {
    members = position.getMembers()
    for member \in members {
      if (member.Type == Measure) {
        projections.add(member)
      }
    }
  }
  return new LdOlapQuery(cube, slicesrollups, dices, projections)

5) SPARQL query

Algorithm 1: OLAP Query Processing
Input: An OLAP Query (cube, SlicesRollups, Dices, Projections)
Output: A SPARQL query string

begin
  whereClause = graph pattern for observation in dataset described by data structure definition of cube
  for level \in SlicesRollups AND level != ALL do
    selectClause = selectClause + select for level
    levelHeight = level.getHeight()
    dimension = level.getHierarchy().getDimension()
    hashMap.put(dimension, levelHeight)
    for 0 to levelHeight do
      levelPath = levelPath + graph pattern for path via skos:narrower 
    levelPath = levelPath + graph pattern for level member via skos:member
    whereClause = whereClause + levelPath
    groupByClause = groupByClause + group by level
  // We assume that each position has the same metadata (i.e., Levels)
  position = Dices.positions.get(0)
  for member \in position {
    dicesLevelHeight = member.getLevel().getHeight()
    slicesRollupLevelHeight = hashMap.get(member.getLevel().getHierarchy().getDimension()
    levelPath = ""
    for 0 to slicesRollupLevelHeight - dicesLevelHeight {
      levelPath = levelPath + graph pattern for path via skos:narrower
    }
  } 
  for position \in Dices {
    for member \in Members do
      memberFilterAnd = memberFilterAnd + "AND" + filter for member
    memberFilter = memberFilter + "OR" + memberFilterAnd 
  }
  whereClause = whereClause + levelPath + memberFilter
  for measure \in Projections do
    aggregationSelect = measure.getAggregationFunction()
    selectClause = selectClause + aggregationSelect
    optionalPattern = optional graph pattern with measure
    whereClause = whereClause + optionalPattern
  query = selectClause + whereClause + groupByClause
  return query




==2013-02-07==
* Added to Tomcat for executing Saiku: "set CATALINA_OPTS=-Xms512m -Xmx768m -XX:MaxPermSize=256m -Dfile.encoding=UTF-8 
-Dorg.apache.tomcat.util.buf.UDecoder.ALLOW_ENCODED_SLASH=true" , as explained here: 
http://ask.analytical-labs.com/questions/943/saiku-dont-execute-in-the-browser?focusedAnswerId=1143&sort=votes&page=2

* Logging refactored: Logging is enabled/disabled in LdOlap4jDriver.java (line 193):
** Disabled: LdOlap4jUtil._log.setLevel(Level.SEVERE);
** Enabled: LdOlap4jUtil._log.setLevel(Level.INFO);

===Problem: LinkedDataEngine may be called many times synchronously===
* So far, we have one LinkedDataEngine per Connection. 
* However, several metadata lists may get populated at the same time. Also, Saiku may be used by
several people at the same time. 
* Currently, the LinkedDataEngine has a state, since it has the current context stored as object
properties. 
* Solution: We create an internal class similar to "context", that stores the needed names from 
restrictions.  

===Problem: Created MDX in Saiku does not surround entity names with square brackets===
* The MDX parser requires entity names to be wrapped by square brackets.
* So far, only a cubes unique name is wrapped by square brackets. 
* All other elements are defined by XmlaOlap4jElement and they do not add square brackets.
* To consider
** Unique names of elements are used for filtering in SPARQL queries.
* Possible Solution: We just add the brackets as "MDX"-specific. 
** However, we should only transform SPARQL outputs if they are needed in MDX.
* Next problem: From segment identifier objects, the [] are already removed.
* 
* Any possibility to know the cube when we are looking for an identifier?

===Problem: How about tree operators===
* Currently, if tree operators are used, we assume that we are looking for a measure member.
* Now that we do not have applyRestrictions any more, it becomes more difficult.
* The following treeOps are possible:
** Interesting: 
	 * <p>The <code>treeOps</code> parameter allows you to retrieve members
     * relative to a given member. It is only applicable if a
     * <code>memberUniqueName</code> is also specified; otherwise it is
     * ignored. The following example retrieves all descendants and ancestors
     * of California, but not California itself:
** Parent, Siblings, Children, Self, Ancestors, Descendants

==2013-02-03==
* Problem: If we run a test on data sources with metadata queries, olap4ld does not yet know
how to translate prefixes to URIs. If I run OLAP queries, it does not since it populates the object
metadata and gives names to all multidimensional elements. However, for metadata queries, it does
not do it. After a metadata query is issued, for output, the elements are translated to MDX 
expressions, i.e., after output, the prefixes are known. That means, for metadata queries, we have 
to first query for cubes, then dimensions, then hierarchies etc. 

* Solution: We use prefixes as mentioned in the standardprefixes.csv. If we do not find prefixes,
we encode the URLs ourselves. This will not be pretty, but for those reasons we need labels.  
* Next solution: We now use Base64, since it would completely encode a URI and would hopefully 
not leave any problematic characters (http://blog.axxg.de/2012/02/java-kodierung-base64/)
* For that, we had to add apache commons (http://commons.apache.org/codec/apidocs/overview-summary.html)
* Note: Base64 had to be used with 0 linelength so that it would not add a linebreak.
* Encoding/Decoding yourself at: http://www.yellowpipe.com/yis/tools/encrypter/index.php

==2013-01-31==
* Measures and aggregation functions: We say that the aggregation function of a measure is inherent 
to the measure, i.e., it has to define itself how it aggregates if viewed from a lower granularity.
From a measure, you can always derive new measures.   

==2013-01-19==
* When filtering for members in SSB, I had to use function str(), otherwise I got an error "Virtuoso 22005 Error SR130 Bad type VARCHAR".

==2013-01-17==
* getSchemas and getCatalogs return different columns than previously used. See API.
* Wildcards disabled: http://www.w3schools.com/sql/sql_wildcards.asp

==2012-03-28==
* Encoding all to UTF-8
* Testing now works
* Implementing of SPARQL concept 

==2012-01-31==
* License added
* CROSSJOIN added

==2012-01-29==
* Add ordering for getXXX methods in LinkedDataEngine, according to [1]
* Add handling of names
* TODO: Clean up (old methods, VisitorClasses...) 


[1] <http://olap4j.svn.sourceforge.net/viewvc/olap4j/trunk/doc/olap4j_fs.html#The_OlapDatabaseMetaData_interface_and_methods_which_return_schema_rowsets>

==2012-01-28==
* Tested with Saiku-2.2RC1
* Filter axis supported
* Single measures supported

==2012-01-22==
* OLAP4LD does not require mondrian or MySQL server, anymore.
* No results are given, yet.

==2011-12-12==
* README added

==2011-11-21==
* Supports OpenVirtuoso triple store
